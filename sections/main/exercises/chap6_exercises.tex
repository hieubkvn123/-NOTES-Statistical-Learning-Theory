\begin{exercise}{}{exercise_6.1}
    Can you improve the constants in the empirical Rademacher complexity bound:
    \begin{align*}
        \sup_{g\in\G}\biggCurl{\E\bigSquare{g(Z)} - \frac{1}{n}\sum_{i=1}^n g(Z_i)} \le 2\ERC_S(\G) + 3(b-a)\sqrt{
            \frac{\log 2/\delta}{2n}
        }
    \end{align*}
    \noindent through a single, direct application of the bounded diï¬€erence inequality?
\end{exercise}

\begin{solution*}[Exercise \ref{ex:exercise_6.1} - \color{red}{Wrong, to be fixed later} \color{black}]
    Given the sample $S=\{Z_1, \dots, Z_i, \dots, Z_n \}$ and define $S_i = \{Z_1, \dots, Z_i', \dots, Z_n \}$ for $1\le i \le n$. Define the following function:
    \begin{align*}
        \phi(S) &= \widehat{\E_S}[g] - \ERC_S(\G) \\
            &= \frac{1}{n}\sum_{i=1}^n g(Z_i) - \E_\sigma\biggSquare{
                \sup_{g\in\G}\frac{1}{n} \sum_{i=1}^n \sigma_i g(Z_i)
            }
    \end{align*}

    \noindent Note that we have:
    \begin{align*}
        \E[\phi(S)] &= \E\bigSquare{
            \widehat{\E_S}[g] - \ERC_S(\G)
        } = \E[g] - \RC_n(\G)
    \end{align*}

    \noindent We now check the bounded difference property of $\phi(S)$, for $1\le i \le n$, we have:
    \begin{align*}
        |\phi(S) - \phi(S_i)| &= \biggAbs{
            \frac{1}{n}\bigRound{g(Z_i) - g(Z_i')} - \E_\sigma\biggSquare{
                \sup_{g\in\G} \frac{1}{n}\sigma_i\bigRound{g(Z_i) - g(Z_i')}
            }
        } \\
        &= \biggAbs{
            \frac{1}{n}\bigRound{g(Z_i) - g(Z_i')} - \E_\sigma\biggSquare{
                \sup_{g\in\G} \frac{1}{n}\bigRound{g(Z_i) - g(Z_i')}
            }
        } \ \ \ (\text{Since $\sigma_i\in\{-1, 1\}$}) \\
        &\le 2\cdot\frac{b-a}{n}
    \end{align*}

    \noindent Hence, by the bounded difference inequality, let $\epsilon > 0$, we have:
    \begin{align*}
        P\bigRound{
            \phi(S) - \E[\phi(S)] \ge \epsilon
        } &\le \exp\biggRound{
            -\frac{2\epsilon^2}{\sum_{i=1}^n 4(b-a)^2/n^2}
        } \\
        &= \exp\biggRound{
            -\frac{n\epsilon^2}{2(b-a)^2}
        }
    \end{align*}

    \noindent Letting $\delta = \exp\biggRound{-\frac{n\epsilon^2}{2(b-a)^2}}$, we have:
    \begin{align*}
        \epsilon &= (b-a)\sqrt{
            \frac{2\log 1/\delta}{n}
        }
    \end{align*}

    \noindent Therefore, with probability of at least $1-\delta$ ($\delta>0$), we have:
    \begin{align*}
        \phi(S) - \E[\phi(S)] &\le (b-a)\sqrt{
            \frac{2\log 1/\delta}{n}
        } \\
        \implies
        \widehat{\E_S}[g] - \E[g] &\le (\ERC_S(\G) - \RC_n(\G)) + (b-a)\sqrt{
            \frac{2\log 1/\delta}{n}
        }
    \end{align*}
\end{solution*}

\begin{exercise}{}{exercise_6.2}
    \textbf{Definition} (Partition classifier) : Let $\Pi=\bigCurl{A_1, \dots, A_k}$ be a fixed partition of $\X$. The partition classifiers assign labels that corresponds to the partitions. Specifically, we define the following set of classifiers:
    \begin{align*}
        \Hf = \biggCurl{
            h : \X \to \{a_1, \dots, a_k\} \Bigg| h(x) = \sum_{j=1}^k a_j \1{x\in A_j}, \ a_j \in \{-1,1\}
        }
    \end{align*}

    \noindent Then we have $|\Hf|=2^k$. Derive the exact empirical Rademacher complexity of $\Hf$.
\end{exercise}

\begin{solution*}[Exercise \ref{ex:exercise_6.2}]
    We have:
    \begin{align*}
        \ERC_S(\Hf) 
            &= \E_\sigma\biggSquare{
                \sup_{h\in \Hf} \frac{1}{n} \sum_{i=1}^n \sigma_i h(X_i)
            } \\
            &= \frac{1}{n} \E_\sigma\biggSquare{
                \sup_{a_1, \dots, a_k} \sum_{i=1}^n \sigma_i \sum_{j=1}^k a_j \1{X_i\in A_j}
            } \\
            &= \frac{1}{n} \E_\sigma\biggSquare{
                \sup_{a_1, \dots, a_k} \sum_{j=1}^k \sum_{i=1}^n \sigma_i a_j \1{X_i\in A_j}
            } \\
            &= \frac{1}{n} \E_\sigma\biggSquare{
                \sup_{a_1, \dots, a_k} \sum_{j=1}^k \sum_{i\in[n]; X_i \in A_j} \sigma_i a_j
            } \\
            &= \frac{1}{n} \E_\sigma\biggSquare{
                \sum_{j=1}^k \sup_{a_j\in\{-1, 1\}} \sum_{i\in[n]; X_i \in A_j} \sigma_i a_j
            } \\
            &= \frac{1}{n}\sum_{j=1}^k \E_\sigma\biggSquare{
                \sup_{a_j\in\{-1, 1\}} a_j \sum_{i\in[n]; X_i \in A_j} \sigma_i
            }
    \end{align*}

    \noindent Given a sample of Rademacher variables, to maximize the sum of those variables, the strategy is to multiply the entire sample with the dominant sign. For example,
    \begin{align*}
        \{-1, -1, 1\} &\to \{1, 1, -1\} &&\Sigma = 1 \\
        \{1, 1, -1\} &\to \{1, 1, -1\} &&\Sigma = 1
    \end{align*}

    \noindent This is equivalent to taking the absolute value of the sum. Hence, we have:
    \begin{align*}
        \ERC_S(\Hf) 
            &= \frac{1}{n}\sum_{j=1}^k \E_\sigma\biggSquare{
                \biggAbs{
                    \sum_{i\in[n]; X_i \in A_j} \sigma_i
                }
            } \\
            &= \frac{1}{n}\sum_{j=1}^k \E_\sigma\biggSquare{
                \biggAbs{
                    \sum_{i \in \mathcal{N}_j} \sigma_i
                }
            }, \ \ \ \mathcal{N}_j = \bigCurl{ i : X_i \in A_j } 
    \end{align*}

    \noindent We can model $\bigAbs{\sum_{i\in \mathcal{N}_j}\sigma_i}$ as the absolute difference between two binomial variables with distribution $Binomial(n_j, p=\frac{1}{2})$ where $n_j=|\mathcal{N}_j|$. For each $j\in \{1, \dots, k\}$, denote that:
    \begin{align*}
        \biggAbs{
            \sum_{i \in \mathcal{N}_j} \sigma_i
        } &= \bigAbs{ X_j - Y_j } \text{ where } X_j, Y_j \sim Binomial\Bigg(n_j, p=\frac{1}{2}\Bigg)
    \end{align*}

    \begin{subproof}{Compute $P\bigRound{\bigAbs{X_j - Y_j} = z}$ for $z\ne 0$}
        For the case when $X_j \ne Y_j$, we have:
        \begin{align*}
            P\bigRound{\bigAbs{X_j - Y_j} = z} 
                &= P\bigRound{X_j = z + Y_j} + P\bigRound{Y_j = z + X_j}
        \end{align*}

        \noindent Since the two events are independent and has the same probability (due to the fact that $X_j$ and $Y_j$ are identically distributed). By the law of total probability, we have:
        \begin{align*}
            P\bigRound{X_j = z + Y_j}
                &= \sum_{y=0}^{n_j - z} P(X_j = z + Y_j | Y_j = y)P(Y_j = y) \\
                &= \sum_{y=0}^{n_j - z} \biggSquare{\begin{pmatrix} n_j \\ z + y \end{pmatrix} \frac{1}{2^{n_j}}} \biggSquare{ \begin{pmatrix} n_j \\ y \end{pmatrix} \frac{1}{2^{n_j}} } \\
                &= \frac{1}{2^{2n_j}}\sum_{y=0}^{n_j-z} \begin{pmatrix} n_j \\ y \end{pmatrix} \begin{pmatrix} n_j \\ z + y \end{pmatrix} \\
                &= \frac{1}{2^{2n_j}}\sum_{y=0}^{n_j-z} \begin{pmatrix} n_j \\ y \end{pmatrix} \begin{pmatrix} n_j \\ n_j - z - y \end{pmatrix} \\
                &= \frac{1}{2^{2n_j}}\begin{pmatrix}
                    2n_j \\ n_j - z
                \end{pmatrix} \ \ \ \text{(Vandermonde's Identity)}
        \end{align*}

        \noindent Hence, for $X_j\ne Y_j$, we have:
        \begin{align*}
            P\bigRound{\bigAbs{X_j - Y_j} = z} &= 2 \times \frac{1}{2^{2n_j}}\begin{pmatrix}
                2n_j \\ n_j - z
            \end{pmatrix} \\
            &= \frac{1}{2^{2n_j-1}}\begin{pmatrix}
                2n_j \\ n_j - z
            \end{pmatrix}
        \end{align*}
    \end{subproof}

    \begin{subproof}{Compute $\E\bigSquare{\bigAbs{X_j - Y_j}}$}
        Now that we have the PMF for $\bigAbs{X_j - Y_j}$, we have:
        \begin{align*}
            \E\bigSquare{\bigAbs{X_j - Y_j}}
                &= \sum_{z=1}^{n_j} z \times P\bigRound{\bigAbs{X_j - Y_j} = z} \\
                &= \frac{1}{2^{2n_j - 1}} \sum_{z=1}^{n_j} z \times \begin{pmatrix}
                    2n_j \\ n_j - z
                \end{pmatrix}
        \end{align*}
    \end{subproof}

    \noindent Now, plug the above into the formula of the empirical Rademacher Complexity, we have:
    \begin{align*}
        \ERC_S(\Hf)
            &= \frac{1}{n}\sum_{j=1}^k \E_\sigma\biggSquare{\biggAbs{
                \sum_{i\in\mathcal{N}_j}\sigma_i
            }} \\
            &= \frac{1}{n}\sum_{j=1}^k \E\bigSquare{\bigAbs{
                X_j - Y_j
            }} \\
            &= \frac{1}{n}\sum_{j=1}^k \frac{1}{2^{2n_j - 1}} \sum_{z=1}^{n_j} z \times \begin{pmatrix}
                2n_j \\ n_j - z
            \end{pmatrix}
    \end{align*}
\end{solution*}


\begin{exercise}{}{exercise_6.3}
    Let $\G, \G_1, \G_2 \subset [a,b]^{\mathcal{Z}}$. Let $c, d \in \R$. Show that:
    \begin{itemize}
        \item $\ERC_S(c\G + d) = |c|\ERC_S(\G)$ where $c\G+d=\bigCurl{g'(z) = c\cdot g(z) + d \Big| g\in\G}$.
        \item $\ERC_S(conv(\G))=\ERC_S(\G)$ where $conv(\G)=\bigCurl{\sum_{i=1}^n \alpha_ig_i\Big|\alpha_i\ge0, \sum_{i=1}^n\alpha_i=1, g_i\in\G}$.
        \item $\ERC_S(\G_1 + \G_2)=\ERC_S(\G_1) + \ERC_S(\G_2)$ where $\G_1 + \G_2 = \bigCurl{g_1 + g_2 \Big| g_1\in\G_1, g_2\in\G_2}$.
    \end{itemize}
\end{exercise}

\begin{solution*}[Exercise \ref{ex:exercise_6.3}]
    Proving each property one by one, we have:
    \newline
    \begin{subproof}{${\bf (i)}$ Prove that $\ERC_S(c\G + d) = |c|\ERC_S(\G)$ \newline where $c\G+d=\bigCurl{g'(z) = c\cdot g(z) + d \Big| g\in\G}$.}
        Given a sample $S=\{Z_1, \dots, Z_n\}$. Denote $\G' = c\G + d$, we have:
        \begin{align*}
            \ERC_S(\G') 
                &= \E_\sigma\biggSquare{
                    \sup_{g'\in\G'} \frac{1}{n} \sum_{i=1}^n \sigma_i g'(Z_i)
                } \\
                &= \E_\sigma\biggSquare{
                    \sup_{g\in\G}\sum_{i=1}^n \sigma_i\bigRound{c\cdot g(Z_i) + d}
                } \\
                &= \E_\sigma\biggSquare{
                    \sup_{g\in\G}\sum_{i=1}^n \sigma_i \bigRound{c\cdot g(Z_i)}
                } + \E_\sigma\biggSquare{d\cdot\sum_{i=1}^n \sigma_i} \\
                &= |c|\cdot \E_\sigma\biggSquare{
                    \sup_{g\in\G}\sum_{i=1}^n \sigma_i g(Z_i)
                } + |d|\cdot\underbrace{\E_\sigma\biggSquare{\sum_{i=1}^n \sigma_i}}_{=0} \ \ \ \text{(Since $\sigma_i$'s are symmetric)} \\
                &= |c|\cdot \E_\sigma\biggSquare{
                    \sup_{g\in\G}\sum_{i=1}^n \sigma_i g(Z_i)
                } = |c|\cdot \ERC_S(\G)
        \end{align*}
    \end{subproof}

    \begin{subproof}{\newline ${\bf (ii)}$ Prove that $\ERC_S(\G_1 + \G_2)=\ERC_S(\G_1) + \ERC_S(\G_2)$ \newline where $\G_1 + \G_2 = \bigCurl{g_1 + g_2 \Big| g_1\in\G_1, g_2\in\G_2}$.}
        \begin{align*}
            \ERC_S(\G_1 + \G_2)
                &= \E_\sigma\biggSquare{
                    \sup_{g_1\in\G_1, g_2\in\G_2} \frac{1}{n} \sum_{i=1}^n \sigma_i \cdot (g_1(Z_1) + g_2(Z_2))
                } \\
                &= \E_\sigma\biggSquare{
                    \sup_{g_1\in\G_1}\frac{1}{n} \sum_{i=1}^n \sigma_i g_1(Z_i)
                } + \E_\sigma\biggSquare{
                    \sup_{g_2\in\G_2}\frac{1}{n} \sum_{i=1}^n \sigma_i g_2(Z_i)
                } \\
                &= \ERC_S(\G_1) + \ERC_S(\G_2)
        \end{align*}
    \end{subproof}

    \begin{subproof}{\newline ${\bf (iii)}$ Prove that $\ERC_S(conv(\G))=\ERC_S(\G)$ \newline where $conv(\G)=\bigCurl{\sum_{i=1}^n \alpha_ig_i\Big|\alpha_i\ge0, \sum_{i=1}^n\alpha_i=1, g_i\in\G}$.}
        We can rewrite $conv(\G)$ as:
        \begin{align*}
            conv(\G) = \bigcup_{\{\alpha_1, \dots, \alpha_n\}\subset\R_+, \sum_{i=1}^n\alpha_i=1} \biggCurl{\sum_{i=1}^n \alpha_i \G}
        \end{align*}

        \noindent Hence, for any set of $\{\alpha_1, \dots, \alpha_n\}\subset \R_+$ such that $\sum_{i=1}^n \alpha_i =1$, we have:
        \begin{align*}
            \ERC_S(conv(\G)) &= \sum_{i=1}^n |\alpha_i| \ERC_S(\G) \\
                &= \ERC_S(\G) \sum_{i=1}^n \alpha_i \\
                &= \ERC_S(\G)
        \end{align*}
    \end{subproof}
\end{solution*}

