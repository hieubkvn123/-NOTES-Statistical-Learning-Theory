\subsection{End of chapter exercises}
\begin{exercise}{}{exercise_3.1}
    \begin{itemize}
        \item $\bf (i)$ Apply Chernoff's bounding method to obtain an exponential bound on the tail probability $P(Z\ge t)$ for a Gaussian random variable $Z\sim\mathcal{N}(\mu, \sigma^2)$.
        \item $\bf (ii)$ Appealing to the central limit theorem, use part $\bf (i)$ to give an approximate bound on the binomial tail. This should not only match the exponential decay given by Hoeffdingâ€™s inequality, but also reveal the dependence on the variance of the binomial.
    \end{itemize}
\end{exercise}

\begin{solution*}[Exercise \ref{ex:exercise_3.1}]
    .
    \begin{subproof}{$\bf (i)$ Chernoff's bounds for $Z\sim\mathcal{N}(\mu, \sigma^2)$}
        Using the Chernoff's bounding method, we have:
        \begin{align*}
            P(Z\ge t) 
                &\le \inf_{s>0}e^{-st}M_Z(s) \\
                &= \inf_{s>0}\exp\biggRound{
                    -st + \mu s + \frac{1}{2}\sigma^2 s^2
                } 
        \end{align*}

        \noindent The above bound is the tightest when the derivative of the term inside the exponential equals zero. Hence, we have:
        \begin{align*}
            -t + \mu + s\sigma^2 = 0 \implies s = \frac{t-\mu}{\sigma^2}
        \end{align*}
        \noindent From the above, we have the tightest Chernoff's bound as followed:
        \begin{align*}
            P(Z\ge t) \le \exp\biggRound{
                -\frac{(t-\mu)^2}{\sigma^2} + \frac{(t-\mu)^2}{2\sigma^2}
            } = \exp\biggRound{
                -\frac{(t-\mu)^2}{2\sigma^2}
            }
        \end{align*}
    \end{subproof}

    \begin{subproof}{$\bf (ii)$ Binomial tail upper bound}
        Let $S_n$ be the binomial random variable such that:
        \begin{align*}
            S_n = \sum_{i=1}^n X_i, \ \ X_i \sim Bernoulli(p)
        \end{align*}

        \noindent For a positive $\epsilon > 0$, we want to know the upper tail bound $P(S_n - \mathbb{E}[S_n] \ge\epsilon)$. Letting $\overline{X} = \frac{1}{n}S_n$, we have:
        \begin{align*}
            P(S_n - \mathbb{E}[S_n] \ge\epsilon) 
                &= P\biggRound{
                    \overline{X} - \frac{\mathbb{E}[S_n]}{n} \ge \frac{\epsilon}{n}
                }  \\
                &= P\biggRound{
                    \overline{X} - p \ge \frac{\epsilon}{n}
                }  \\
                &= P\biggRound{
                    \frac{\overline{X} - p}{\sqrt{pq}/\sqrt{n}} \ge \frac{\epsilon}{\sqrt{npq}}
                }, \ \ \ (q = 1-p)
        \end{align*}

        \noindent By the Central Limit Theorem, we have:
        \begin{align*}
            \frac{\overline{X} - p}{\sqrt{pq}/\sqrt{n}} \xrightarrow{d} \mathcal{N}(0,1)
        \end{align*}

        \noindent Hence, as $n\to\infty$, the upper tail bound would be:
        \begin{align*}
            P(S_n - \mathbb{E}[S_n] \ge\epsilon) 
                &= P\biggRound{
                    \frac{\overline{X} - p}{\sqrt{pq}/\sqrt{n}} \ge \frac{\epsilon}{\sqrt{npq}}
                } \\
                &\le \exp\biggRound{ -\frac{\epsilon^2}{2npq} } = \exp\biggRound{-\frac{\epsilon^2}{2Var(S_n)}}
        \end{align*}

        \noindent Double-check the bound with Hoeffding's inequality, we have:
        \begin{align*}
            P(S_n - \mathbb{E}[S_n] \ge\epsilon)  \le \exp\biggRound{-\frac{2\epsilon^2}{n}}
        \end{align*}
    \end{subproof}
\end{solution*}


\begin{exercise}{}{exercise_3.2}
    Can you remove the assumption in $0 < \alpha \le p_y(x)$? Consider other restrictions on $p_y$,
    other concentration inequalities, or other $f$-divergences.
\end{exercise}

\begin{solution*}[Exercise \ref{ex:exercise_3.2}]
    When we remove the assumption that $0 < \alpha \le p_y(x)$, the class-conditional densities are not bounded below. Hence, we have:
    \begin{align*}
        \exp\biggRound{
                -\frac{2nD(p_1||p_0)^2}{4(\log\beta - \log\alpha)^2}
        } \to 1 \text{ when } \alpha \to 0
    \end{align*}

    \noindent In other words, the bound is no longer meaningful. We can instead use the Chernoff bounding method:
    \begin{align*}
        R_0(\widehat{h_n}) 
            &= P(S_n \ge 0 | Y = 0) \\
            &\le \inf_{s>0} \prod_{i=1}^n \mathbb{E}_{q_0}\bigSquare{e^{sZ_i}} \\
            &= \inf_{s>0} \prod_{i=1}^n \mathbb{E}_{q_0}\biggSquare{
                \exp\biggRound{
                    s\log\frac{p_1(X_i)}{p_0(X_i)}
                }
            } \\
            &= \inf_{s>0} \prod_{i=1}^n \mathbb{E}_{q_0}\biggSquare{
                \frac{p_1(X_i)^s}{p_0(X_i)^s}
            } \\
    \end{align*}

    \noindent Taking logarithm from both sides, we have:
    \begin{align*}
        \log R_0(\widehat{h_n}) 
            &\le \inf_{s>0} \sum_{i=1}^n \log \mathbb{E}_{q_0}\biggSquare{
                \frac{p_1(X_i)^s}{p_0(X_i)^s}
            } \\
            &= \inf_{s>0}  \sum_{i=1}^n (s-1)R_s(p_1 || p_0) \\
            &= \inf_{s>0}  n(s-1)R_s(p_1 || p_0) \\
    \end{align*}

    \noindent Where $R_s(p_1||p_0)$ is the Renyi divergence \cite{wiki:renyi_entropy}.
\end{solution*}