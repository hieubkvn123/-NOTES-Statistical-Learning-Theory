\newpage
\subsection{Rademacher Complexity of the Ramp loss function}

\subsubsection{Problem statement}
\textbf{Problem} : Consider the multi-class classification problem with $K$ labels ($K\ge2$). Given the following function class
\begin{align*}
    W  &= \biggCurl{
        \vecbf{w}\in \R^{K\times d} \Bigg| \|\vecbf{w}\|_F \le R
    } \\
    \F &= \biggCurl{
        f_{\vecbf{w}}:\R^d \to \R^{K} \Bigg| f_\vecbf{w}(\vecbf{x}) = \vecbf{wx}; \vecbf{w}\in W, \|x\|_2\le 1
    }
\end{align*}

\noindent and consider the following loss function:
\begin{align*}
    l_r(\vecbf{z}) &= \begin{cases}
        0 & \text{if } \vecbf{z} \ge r \\
        1 - \vecbf{z}/r &\text{if } \vecbf{z} \in (0,r) \\ 
        1 &\text{if } \vecbf{z} \le 0
    \end{cases} \\
\end{align*}

% \noindent Let $\epsilon > 0$ and $\mathcal{C}_\epsilon(W)$ be the minimum $\epsilon$-cover of $W$ with respect to the Frobenius norm $\|.\|_F$. Suppose that for all $1\le k \le K$, we have:
\noindent Derive the bound for the Rademacher complexity of the loss function class $\RC_n(\mathcal{L}_r)$ where:
\begin{align*}
    \mathcal{L}_r = \biggCurl{
        (x, y)\mapsto l_r\bigRound{ f_\vecbf{w}(x)_y - \max_{k\ne y} f_\vecbf{w}(x)_k } \Bigg| f_{\vecbf{w}} \in \F 
    }
\end{align*}

\subsubsection{Rough proof sketch}
\textbf{Bounding the covering number of $\mathcal{L}_r$} : Let $L_\vecbf{w} \in \mathcal{L}_r$ where
\begin{align*}
    L_\vecbf{w}(x,y) =  l_r\bigRound{ f_\vecbf{w}(x)_y - \max_{k\ne y} f_\vecbf{w}(x)_k }
\end{align*}


\noindent and $S=\bigCurl{(x_i, y_i)}_{i=1}^n$ be a sample dataset. Define the following norm:
\begin{align*}
    \|L_\vecbf{w}\|_{\mathcal{L}_r} = \sqrt{
        \frac{1}{n} \sum_{i=1}^n L_\vecbf{w}(x_i, y_i)^2
    }
\end{align*}

\noindent We have to find an $\epsilon$-covering with respect to $\|.\|_{\mathcal{L}_r}$, denoted as $\mathcal{C}_\epsilon({\mathcal{L}_r}_{|_S}, \|.\|_{\mathcal{L}_r})$. Meaning, $\forall L_\vecbf{w}\in\mathcal{L}_r$, $\exists L_\vecbf{\bar w} \in \mathcal{C}_\epsilon(\mathcal{L}_r, \|.\|_{\mathcal{L}_r})$ such that:
\begin{align*}
    \|L_\vecbf{w} - L_\vecbf{\bar w}\|_{\mathcal{L}_r} = \sqrt{
        \frac{1}{n} \sum_{i=1}^n \bigRound{
            L_\vecbf{w}(x_i, y_i) - L_\vecbf{\bar w}(x_i, y_i)
        }^2
    } \le \epsilon
\end{align*}

\noindent\textbf{Remark 1} : Bounding each component.\newline
\noindent For any $f_\vecbf{w}\in\F$, we have:
\begin{align*}
    f_\vecbf{w} = \begin{pmatrix}
        f_\vecbf{w}^{(1)}(x) \\
        \vdots \\
        f_\vecbf{w}^{(K)}(x)
    \end{pmatrix}
    = 
    \begin{pmatrix}
        \vecbf{w}_{1}x \\
        \vdots \\
        \vecbf{w}_{K}x
    \end{pmatrix}
\end{align*}

\noindent Where $\vecbf{w}_j$ is the $j^{th}$ row of $\vecbf{w}$ for $j=\{1, \dots, K\}$. Define the following classes:
\begin{align*}
    \F_j = \bigCurl{f_\vecbf{w}^{(j)} : f_\vecbf{w} \in \F} = \bigCurl{\vecbf{x} \mapsto \vecbf{w}_j\vecbf{x} : \vecbf{w} \in W}, \ \ j \in \{1, \dots, K\}
\end{align*}

\noindent By theorem 4 in \cite{article:tong_zhang}, we have:
\begin{align*}
    \log_2\mathcal{N}\bigRound{
        \G_{|S}, \epsilon, \|.\|_\infty
    } &\le \frac{36a^2b^2}{\epsilon^2}\log_2\biggRound{
        \frac{8ab}{\epsilon}n + 6n + 1
    } \\
    \text{Where : } \G &=  \bigCurl{
        \vecbf{x} \to \vecbf{wx} : \|\vecbf{x}\|_2 \le b, \|\vecbf{w}\|_2 \le a
    }
\end{align*}

\noindent Where for a sample dataset $S$, we define the $\|.\|_\infty$ for any $g\in \G$ as:
\begin{align*}
    \|g\|_\infty = \sup_{x_i \in S} |x_i|
\end{align*} 

\noindent Since for any $\vecbf{w}\in W$, we have $\|\vecbf{w}\|_F\le R$. Therefore, for all $j\in\{1, \dots, K\}$, we have $\|\vecbf{w}_j\|_2\le R$. Hence,
\begin{align*}
    \log_2\mathcal{N}\bigRound{{F_j}_{|S}, \epsilon, \|.\|_\infty} \le \frac{36R^2}{\epsilon^2}\log_2\biggRound{\frac{8Rn}{\epsilon} + 6n + 1}
\end{align*}

\noindent \color{red}As convenient as this is, we still have to bound all components of the outputs of functions in $\F$.\color{black}

\noindent\textbf{\newline Remark 2} : Bounding all components.\newline
\noindent Let $\vecbf{w}, \vecbf{\bar w}\in W$. For any pair $(x_i, y_i) \in S$, we have:
\begin{align*}
    \bigAbs{L_\vecbf{w}(x_i, y_i) - L_\vecbf{\bar w}(x_i, y_i)}
        &= \bigAbs{
            l_r\bigRound{ f_\vecbf{w}(x_i)_{y_i} - \max_{k\ne y_i} f_\vecbf{w}(x_i)_k } - l_r\bigRound{ f_\vecbf{\bar w}(x_i)_{y_i} - \max_{k\ne y_i} f_\vecbf{\bar w}(x_i)_k }
        } \\
        &\le \frac{1}{r}\bigAbs{
            \bigRound{f_\vecbf{w}(x_i)_{y_i} - f_\vecbf{\bar w}(x_i)_{y_i}} + \bigRound{\max_{k\ne y_i} f_\vecbf{\bar w}(x_i)_k - \max_{k\ne y_i} f_\vecbf{w}(x_i)_k}
        }\ \ \ (l_r \text{ is } 1/r-\text{Lipchitz}) \\
        &\le \frac{1}{r}\bigAbs{
            \bigRound{f_\vecbf{w}(x_i)_{y_i} - f_\vecbf{\bar w}(x_i)_{y_i}} + \bigRound{\max_{k\ne y_i} \bigCurl{f_\vecbf{\bar w}(x_i)_k - f_\vecbf{w}(x_i)_k}}
        } \\
        &\le \frac{1}{r}\bigAbs{f_\vecbf{w}(x_i)_{y_i} - f_\vecbf{\bar w}(x_i)_{y_i}} + \frac{1}{r}\max_{k\ne y_i}\bigAbs{f_{\vecbf{\bar w}}(x_i)_k - f_\vecbf{w}(x_i)_k} \\
        &\le \frac{2}{r}\sup_{\substack{x_i \in S \\ j \in \{1, \dots, K\}}}\bigAbs{
            f_\vecbf{w}^{(j)}(x_i) - f^{(j)}_\vecbf{\bar w}(x_i)
        } \\
        &= \frac{2}{r}\sup_{j\in\{1, \dots, K\}} \|f^{(j)}_\vecbf{w} - f^{(j)}_\vecbf{\bar w}\|_\infty \\
        &= \frac{2}{r}\max\bigCurl{(\vecbf{w}_j - \vecbf{\bar w}_j)x_i}_{\substack{i\in\{1, \dots, n\}, \\j\in\{1, \dots, K\}}}
\end{align*}

\noindent Now, define the following class of functions:
\begin{align*}
    \Hf &= \bigCurl{
        h:\R^{Kd} \to \R : h(\vecbf{x}) = \beta \vecbf{x}, \beta \in \mathcal{B}
    } \\
    \mathcal{B} &= \bigCurl{
        \beta \in \R^{1\times Kd} : \beta = \begin{pmatrix}
            \vecbf{w}_1 & \dots & \vecbf{w}_K
        \end{pmatrix}, \ \vecbf{w} \in W
    }
\end{align*}

\noindent Basically, we construct the set of parameters $\beta\in \R^{1\times Kd}$ by concatenating rows of vectors $\vecbf{w}\in W$ horizontally. By the definition of $W$, we have that $\|\beta\|_2\le R, \ \forall \beta\in\mathcal{B}$.

\noindent\newline\newline For any sample $S=\{x_i\}_{i=1}^n$ of vectors $x_i\in \R^d$, construct a new sample $S'$ of $\R^{Kd}$ vectors by creating $K$ vectors from each $x_i\in S$:
\begin{align*}
    x_i \longrightarrow \begin{cases}
        x_{i, 1} &= \begin{pmatrix} x_i & \vecbf{0} & \vecbf{0} & \dots & \vecbf{0} \end{pmatrix}^T \\
        x_{i, 2} &= \begin{pmatrix} \vecbf{0} & x_i & \vecbf{0} & \dots & \vecbf{0} \end{pmatrix}^T \\
        &\vdots \\
        x_{i, K} &=\begin{pmatrix} \vecbf{0} & \vecbf{0} & \dots & \vecbf{0} & x_i \end{pmatrix}^T
    \end{cases}
\end{align*}

\noindent For all $i\in \{1, \dots, n\}$ and $j\in\{1, \dots, K\}$, we have $\|x_{i, j}\|_2 \le 1$. Hence, applying Zhang's theorem 4 \cite{article:tong_zhang} on $\Hf_{|S'}$, we have:
\begin{align*}
    \log_2\mathcal{N}\bigRound{\Hf_{|S'}, \epsilon, \|.\|_\infty} \le \frac{36R^2}{\epsilon^2}\log_2\biggRound{
        \frac{8RnK}{\epsilon} + 6nK + 1
    } \le \frac{36R^2}{\epsilon^2}\log_2\biggRound{\biggRound{
        \frac{8R}{\epsilon} + 7}nK
    } 
\end{align*}

