\newpage
\subsection{Rademacher Complexity of ramp loss for neural networks}
\subsubsection{Problem Statement}
\label{sec:rad_complexity_of_ramp_loss_nn}
\textbf{Problem} : Given a tuple $\begin{pmatrix}\M{1} & \dots & \M{L}\end{pmatrix}$ of $L\ge 1$ reference matrices (that represents initial weights in a neural network). Let $\mathcal{A}$ be the class of tuples of $L$ weight matrices defined as followed:
\begin{align*}
    \mathcal{A} = \bigCurl{
        \begin{pmatrix}
            \A{1} & \dots & \A{L}
        \end{pmatrix} : \|\A{i}\|_\sigma \le s_i, \|(\A{i} - \M{i})^T\|_{2, 1}\le a_i, \|\A{L} - \M{L}\|_F \le a_*
    }
\end{align*}

\noindent Where $\|.\|_\sigma$ denotes the spectral norm and $\|.\|_{p, q}$ denotes the matrix $(p, q)$ norm defined as $\|A\|_{p, q} = \bigRound{\sum_j\bigRound{\sum_{i}|A_{ij}|^p}^{q/p}}^{1/q}$. Define the following class of neural networks:
\begin{align*}
    \F_\mathcal{A} = \biggCurl{
        x\mapsto \A{L} \biggRound{
            \bigO_{k=1}^{L-1} \sigma_k \circ \A{k}
        }(x) : \sigma_k \text{ is } \rho_k\text{-Lipchitz}, \begin{pmatrix}
            \A{1} & \dots & \A{L}
        \end{pmatrix} \in \mathcal{A}
    }
\end{align*}

\noindent Derive the bound for the Rademacher Complexity of the loss function class:
\begin{align*}
    \mathcal{L}_r = \biggCurl{
        (x, y) \mapsto l_r\bigRound{
            F_\vecbf{A}(x)_y - \max_{k\ne y}F_\vecbf{A}(x)_k
        } \Bigg| F_\vecbf{A} \in \F_\mathcal{A}
    }
\end{align*}

\noindent Where $l_r$ is the ramp loss function with margin $r\in(0,1)$.

\subsubsection{Neural networks covering bounds with general norm}
\begin{theorem}{Neural networks covering bound with general norm}{nn_cover_bound_with_general_norm}
    Let $L \ge 1$ be a natural number and $\epsilon_1, \dots, \epsilon_L > 0$ be given as covering number granularities. Given the following:
    \begin{itemize}
        \item A sequence of vector spaces $\mathcal{V}_0, \dots, \mathcal{V}_L$ endowed with norms $|.|_0, \dots, |.|_L$.
        \item A sequence of vector spaces $\mathcal{W}_1, \dots, \mathcal{W}_L$ endowed with norms $\|.\|_1, \dots, \|.\|_L$.
        \item A sequence of real positive numbers $c_1, \dots, c_L$ and linear operators $\A{i} : \mathcal{V}_i \to \mathcal{W}_{i+1}$ associated with the operator norm:
        \begin{align*}
            \|\A{i}\|_{op} = \sup_{|Z|_i \le 1} \|A_iZ\|_{i+1} \le c_i, \ \forall i \in \{1, \dots, L\}
        \end{align*}

        \item A sequence of real positive numbers $\rho_1, \dots, \rho_L$ and activation functions $\sigma_i:\mathcal{W}_i \to \mathcal{V}_i$ such that $\sigma_i$ are $\rho_i$-Lipchitz:
        \begin{align*}
            |\sigma_i(z_1) - \sigma_i(z_2)|_i \le \rho_i\|z_1 - z_2\|_i
        \end{align*}

        \item Let $\mathcal{A}\subseteq \mathcal{B}_1\times\dots\times\mathcal{B}_L$ be a class of tuples of matrices $(\A{1}, \dots, \A{L})$ such that each $\A{i} \in \mathcal{B}_i$ satisfies $\|\A{i}\|_{op} \le c_i$.

        \item Define the class of neural networks $\F_\mathcal{A}$ as followed:
        \begin{align*}
            \F_\mathcal{A} = \biggCurl{
                x\mapsto \A{L}\biggRound{
                    \bigO_{k=1}^{L-1}\sigma_k \circ \A{k}
                }(x) : (\A{1}, \dots, \A{L}) \in \mathcal{A}
            }
        \end{align*}
        
        \item Let $\tau$ be the aggregated granularity defined as $\tau = \sum_{j=1}^L \epsilon_j \bigRound{\prod_{l=j+1}^L \rho_lc_l}$ and $Z\subset\mathcal{V}_0$ be a sample dataset. We have:
        \begin{align*}
            \mathcal{N}\bigRound{
                {\F_\mathcal{A}}_{|Z}, \tau, \|.\|_L
            } \le \prod_{i=1}^L \sup_{\substack{
                (\A{1}, \dots, \A{i-1}) \\
                A_j \in \mathcal{B}_j, j \le i-1
            }}\mathcal{N}\bigRound{
                \bigCurl{\A{i}F_\vecbf{\A{1, i-1}}(Z) : \A{i} \in \mathcal{B}_i}, \epsilon_i, \|.\|_i
            }
        \end{align*}
    \end{itemize}
\end{theorem}

\begin{proof*}[Theorem \ref{thm:nn_cover_bound_with_general_norm}]
    \begin{figure}[!ht]
        \centering
        \includegraphics[width=\textwidth]{figures/neural-network.png}
        \caption{Illustration of a feedforward pass of a deep neural network with weight matrices $\vecbf{A}=\Big( \A{1}, \dots, \A{L} \Big)$}
        \label{fig:neural-network-illustration}
    \end{figure}

    \noindent We will prove the above theorem inductively. 
    
    \noindent\newline\textbf{Base case} : Suppose we have a sample dataset $Z\subset\mathcal{V}_0$. Construct a minimum $\epsilon_1$-cover for the weight matrices in the $1^{st}$ layer called $\mathcal{C}_1$ such that for all $\A{1}\in\mathcal{B}_i$, there exists $\bA{1}\in\mathcal{C}_1$ such that:

    \begin{align*}
        \Big\|\A{1}Z - \bA{1}Z\Big\|_1 \le \epsilon_1
    \end{align*} 
    
    
    \noindent \textbf{Inductive step} : For $i, j \ge 2, j \ge i$, define $\vecbf{\A{i, j}}\in\mathcal{B}_i\times\dots\times\mathcal{B}_j$ as the extraction of layers $i$ to $j$ for every $\vecbf{A}\in\mathcal{A}$. Hence, define the function $F_\vecbf{\A{i, j}}$ as followed:
    \begin{align*}
        F_\vecbf{\A{i, j}}(Z) = \sigma_j\bigRound{
            \A{j}\sigma_{j-1}\bigRound{
                \A{j-1}\dots\sigma_i\bigRound{\A{i}Z}\dots
            }
        }
    \end{align*} 
    
    
    \noindent We need to construct a cover $\mathcal{C}_i$ with respect to norm $\|.\|_i$ with granularity $\tau_i>0$ such that for all $\vecbf{\A{1, i}}\in \mathcal{B}_1\times\dots\times\mathcal{B}_i$, there exists $\vecbf{\bA{1, i}}\in \mathcal{C}_{i}$ such that: 
    \begin{align*}
        \Big\|
            \A{i}F_\vecbf{\A{1, i-1}}(Z) - \bA{i}F_\vecbf{\bA{1, i-1}}(Z)
        \Big\|_i \le \epsilon_i
    \end{align*}

    \noindent Suppose that we have constructed covers $\mathcal{C}_{i-1}, \mathcal{C}_{i-2}, \dots, \mathcal{C}_1$ with respect to norms $\|.\|_{i-1}, \|.\|_{i-2}, \dots, \|.\|_1$ with granularities $\tau_{i-1}, \tau_{i-2}, \dots, \tau_1$ (where $\mathcal{C}_1$ is the cover constructed in the base case with granularity $\tau_1=\epsilon_1$). We have:

    \begin{align*}
        \Big\|
            \A{i}F_\vecbf{\A{1, i-1}}&(Z) - \bA{i}F_\vecbf{\bA{1, i-1}}(Z)
        \Big\|_i \\
            &\le \Big\|\bigRound{\A{i} - \bA{i}}F_\vecbf{\bA{1, i-1}}(Z) \Big\|_i 
                + \Big\| \A{i}\bigRound{F_\vecbf{\A{1, i-1}}(Z) - F_\vecbf{\bA{1, i-1}}(Z)}\Big\|_i \\
            &\le \Big\|\bigRound{\A{i} - \bA{i}}F_\vecbf{\bA{1, i-1}}(Z) \Big\|_i 
                + \Big\|\A{i}\Big\|_{op} \cdot\Big| F_\vecbf{\A{1, i-1}}(Z) - F_\vecbf{\bA{1, i-1}}(Z) \Big|_{i-1} \\
            &\le \Big\|\bigRound{\A{i} - \bA{i}}F_\vecbf{\bA{1, i-1}}(Z) \Big\|_i 
                + c_i\Big| F_\vecbf{\A{1, i-1}}(Z) - F_\vecbf{\bA{1, i-1}}(Z) \Big|_{i-1} \\
            &\le \Big\|\bigRound{\A{i} - \bA{i}}F_\vecbf{\bA{1, i-1}}(Z) \Big\|_i 
                + c_i\rho_i\Big\| \A{i-1}F_\vecbf{\A{1, i-2}}(Z) - \bA{i-1}F_\vecbf{\bA{1, i-2}}(Z) \Big\|_{i-1} 
    \end{align*}

    \noindent For each $\vecbf{\bA{1, i-1}}\in \mathcal{C}_{i-1}$, construct the minimum $\epsilon_i$-cover $\mathcal{C}_i(\vecbf{\bA{1, i-1}})$ for the class \newline$\bigCurl{\A{i}F_\vecbf{\bA{1, i-1}}(Z) : \A{i} \in \mathcal{B}_i}$. Then, we have:
    \begin{align*}
        \bigAbs{
            \mathcal{C}_i(\vecbf{\bA{1, i-1}})
        } &\le \sup_{\vecbf{\bA{1, i-1}}\in\mathcal{C}_{i-1}} \mathcal{N}\bigRound{
            \bigCurl{
                \A{i}F_\vecbf{\bA{1, i-1}}(Z) : \A{i}\in\mathcal{B}_i 
            }, \epsilon_i, \|.\|_i
        } \\
        &\le \sup_{\vecbf{\bA{1, i-1}}\in\mathcal{B}_1\times\dots\times\mathcal{B}_{i-1}} \mathcal{N}\bigRound{
            \bigCurl{
                \A{i}F_\vecbf{\bA{1, i-1}}(Z) : \A{i} \in \mathcal{B_i}
            }, \epsilon_i, \|.\|_i
        }
    \end{align*}

    \noindent Now, construct $\mathcal{C}_i$ as followed:
    \begin{align*}
        \mathcal{C}_i = \bigcup_{\vecbf{\bA{1, i-1}}\in\mathcal{C}_{i-1}}
    \end{align*}
\end{proof*}


\subsubsection{Solution to \ref{sec:rad_complexity_of_ramp_loss_nn}}

