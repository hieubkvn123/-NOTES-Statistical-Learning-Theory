\newpage
\subsection{Rademacher Complexity of ramp loss for neural networks}
\subsubsection{Problem Statement}
\label{sec:rad_complexity_of_ramp_loss_nn}
\textbf{Problem} : Given a tuple $\begin{pmatrix}\M{1} & \dots & \M{L}\end{pmatrix}$ of $L\ge 1$ reference matrices (that represents initial weights in a neural network). Let $\mathcal{A}$ be the class of tuples of $L$ weight matrices defined as followed:
\begin{align*}
    \mathcal{A} = \bigCurl{
        \begin{pmatrix}
            \A{1} & \dots & \A{L}
        \end{pmatrix} : \|\A{i}\|_\sigma \le s_i, \|(\A{i} - \M{i})^T\|_{2, 1}\le a_i, \|\A{L} - \M{L}\|_F \le a_*
    }
\end{align*}

\noindent Where $\|.\|_\sigma$ denotes the spectral norm and $\|.\|_{p, q}$ denotes the matrix $(p, q)$ norm defined as $\|A\|_{p, q} = \bigRound{\sum_j\bigRound{\sum_{i}|A_{ij}|^p}^{q/p}}^{1/q}$. Therefore, the $\|.\|_{2, 1}$ is defined as:
\begin{align*}
    \|A\|_{2, 1} = \sum_{j=1}^m \biggRound{
        \sum_{i=1}^n |A_{ij}|^2
    }^{1/2}
    = \sum_{j=1}^m \|A_{., j}\|_2, \ \ A\in\R^{m\times n}
\end{align*}

\noindent Define the following class of neural networks:
\begin{align*}
    \F_\mathcal{A} = \biggCurl{
        x\mapsto \A{L} \biggRound{
            \bigO_{k=1}^{L-1} \sigma_k \circ \A{k}
        }(x) : \sigma_k \text{ is } \rho_k\text{-Lipchitz w.r.t $l_2$ norm}, \begin{pmatrix}
            \A{1} & \dots & \A{L}
        \end{pmatrix} \in \mathcal{A}
    }
\end{align*}

\noindent We denote $d_0, d_1, \dots, d_L$ as the widths of each layer of the neural networks where $d_0$ denotes the dimensionality of the input and $d_L=K$ be the size of the output. 


\noindent Derive the bound for the Rademacher Complexity of the loss function class:
\begin{align*}
    \mathcal{L}_r = \biggCurl{
        (x, y) \mapsto l_r\bigRound{
            F_\vecbf{A}(x)_y - \max_{k\ne y}F_\vecbf{A}(x)_k
        } \Bigg| F_\vecbf{A} \in \F_\mathcal{A}
    }
\end{align*}

\noindent Where $l_r$ is the ramp loss function with margin $r\in(0,1)$.

\subsubsection{Neural networks covering bounds with general norm}
\begin{theorem}{Neural networks covering bound with general norm}{nn_cover_bound_with_general_norm}
    Let $L \ge 1$ be a natural number and $\epsilon_1, \dots, \epsilon_L > 0$ be given as covering number granularities. Given the following:
    \begin{itemize}
        \item A sequence of vector spaces $\mathcal{V}_0, \dots, \mathcal{V}_L$ endowed with norms $|.|_0, \dots, |.|_L$.
        \item A sequence of vector spaces $\mathcal{W}_1, \dots, \mathcal{W}_L$ endowed with norms $\|.\|_1, \dots, \|.\|_L$.
        \item A sequence of real positive numbers $c_1, \dots, c_L$ and linear operators $\A{i} : \mathcal{V}_i \to \mathcal{W}_{i+1}$ associated with the operator norm:
        \begin{align*}
            \|\A{i}\|_{op} = \sup_{|Z|_i \le 1} \|A_iZ\|_{i+1} \le c_i, \ \forall i \in \{1, \dots, L\}
        \end{align*}

        \item A sequence of real positive numbers $\rho_1, \dots, \rho_L$ and activation functions $\sigma_i:\mathcal{W}_i \to \mathcal{V}_i$ such that $\sigma_i$ are $\rho_i$-Lipchitz:
        \begin{align*}
            |\sigma_i(z_1) - \sigma_i(z_2)|_i \le \rho_i\|z_1 - z_2\|_i
        \end{align*}

        \item Let $\mathcal{A}\subseteq \mathcal{B}_1\times\dots\times\mathcal{B}_L$ be a class of tuples of matrices $(\A{1}, \dots, \A{L})$ such that each $\A{i} \in \mathcal{B}_i$ satisfies $\|\A{i}\|_{op} \le c_i$.

        \item Define the class of neural networks $\F_\mathcal{A}$ as followed:
        \begin{align*}
            \F_\mathcal{A} = \biggCurl{
                x\mapsto \A{L}\biggRound{
                    \bigO_{k=1}^{L-1}\sigma_k \circ \A{k}
                }(x) : (\A{1}, \dots, \A{L}) \in \mathcal{A}
            }
        \end{align*}
        
        \item Let $\tau$ be the aggregated granularity defined as $\tau = \sum_{j=1}^L \epsilon_j \bigRound{\prod_{l=j+1}^L c_l\rho_{l-1}}$ and $Z\subset\mathcal{V}_0$ be a sample dataset. We have:
        \begin{align*}
            \mathcal{N}\bigRound{
                {\F_\mathcal{A}}_{|Z}, \tau, \|.\|_L
            } \le |\mathcal{C}_1|\cdot\prod_{i=2}^L \sup_{\substack{
                (\A{1}, \dots, \A{i-1}) \\
                \A{j} \in \mathcal{B}_j, j \le i-1
            }}\mathcal{N}\bigRound{
                \bigCurl{\A{i}F_\vecbf{\A{1, i-1}}(Z) : \A{i} \in \mathcal{B}_i}, \epsilon_i, \|.\|_i
            }
        \end{align*}
    \end{itemize}
\end{theorem}

\begin{proof*}[Theorem \ref{thm:nn_cover_bound_with_general_norm}]
    \noindent We will prove the above theorem inductively. 
    
    \noindent\newline\textbf{Base case} : Suppose we have a sample dataset $Z\subset\mathcal{V}_0$. Construct a minimum $\epsilon_1$-cover for the weight matrices in the $1^{st}$ layer called $\mathcal{C}_1$ such that for all $\A{1}\in\mathcal{B}_i$, there exists $\bA{1}\in\mathcal{C}_1$ such that:

    \begin{align*}
        \Big\|\A{1}Z - \bA{1}Z\Big\|_1 \le \epsilon_1
    \end{align*} 
    
    
    \noindent \textbf{Inductive step} : For $i, j \ge 2, j \ge i$, define $\vecbf{\A{i, j}}\in\mathcal{B}_i\times\dots\times\mathcal{B}_j$ as the extraction of layers $i$ to $j$ for every $\vecbf{A}\in\mathcal{A}$. Hence, define the function $F_\vecbf{\A{i, j}}$ as followed:
    \begin{align*}
        F_\vecbf{\A{i, j}}(Z) = \sigma_j\bigRound{
            \A{j}\sigma_{j-1}\bigRound{
                \A{j-1}\dots\sigma_i\bigRound{\A{i}Z}\dots
            }
        }
    \end{align*} 
    
    
    \noindent We need to construct a cover $\mathcal{C}_i$ with respect to norm $\|.\|_i$ with granularity $\tau_i>0$ such that for all $\vecbf{\A{1, i}}\in \mathcal{B}_1\times\dots\times\mathcal{B}_i$, there exists $\vecbf{\bA{1, i}}\in \mathcal{C}_{i}$ such that: 
    \begin{align*}
        \Big\|
            \A{i}F_\vecbf{\A{1, i-1}}(Z) - \bA{i}F_\vecbf{\bA{1, i-1}}(Z)
        \Big\|_i \le \tau_i
    \end{align*}

    \noindent Suppose that we have constructed covers $\mathcal{C}_{i-1}, \mathcal{C}_{i-2}, \dots, \mathcal{C}_1$ with respect to norms $\|.\|_{i-1}, \|.\|_{i-2}, \dots, \|.\|_1$ with granularities $\tau_{i-1}, \tau_{i-2}, \dots, \tau_1$ (where $\mathcal{C}_1$ is the cover constructed in the base case with \newline granularity $\tau_1=\epsilon_1$). We have:

    \begin{align*}
        \Big\|
            \A{i}F_\vecbf{\A{1, i-1}}&(Z) - \bA{i}F_\vecbf{\bA{1, i-1}}(Z)
        \Big\|_i \\
            &\le \Big\|\bigRound{\A{i} - \bA{i}}F_\vecbf{\bA{1, i-1}}(Z) \Big\|_i 
                + \Big\| \A{i}\bigRound{F_\vecbf{\A{1, i-1}}(Z) - F_\vecbf{\bA{1, i-1}}(Z)}\Big\|_i \\
            &\le \Big\|\bigRound{\A{i} - \bA{i}}F_\vecbf{\bA{1, i-1}}(Z) \Big\|_i 
                + \Big\|\A{i}\Big\|_{op} \cdot\Big| F_\vecbf{\A{1, i-1}}(Z) - F_\vecbf{\bA{1, i-1}}(Z) \Big|_{i-1} \\
            &\le \Big\|\bigRound{\A{i} - \bA{i}}F_\vecbf{\bA{1, i-1}}(Z) \Big\|_i 
                + c_i\Big| F_\vecbf{\A{1, i-1}}(Z) - F_\vecbf{\bA{1, i-1}}(Z) \Big|_{i-1} \\
            &\le \Big\|\bigRound{\A{i} - \bA{i}}F_\vecbf{\bA{1, i-1}}(Z) \Big\|_i 
                + c_i\rho_{i-1}\Big\| \A{i-1}F_\vecbf{\A{1, i-2}}(Z) - \bA{i-1}F_\vecbf{\bA{1, i-2}}(Z) \Big\|_{i-1} 
    \end{align*}

    \noindent\newline For each $\vecbf{\bA{1, i-1}}\in \mathcal{C}_{i-1}$, construct the minimum $\epsilon_i$-cover $\mathcal{C}_i(\vecbf{\bA{1, i-1}})$ for the class \newline$\bigCurl{\A{i}F_\vecbf{\bA{1, i-1}}(Z) : \A{i} \in \mathcal{B}_i}$. Then, we have:
    \begin{align*}
        \bigAbs{
            \mathcal{C}_i(\vecbf{\bA{1, i-1}})
        } &\le \sup_{\vecbf{\bA{1, i-1}}\in\mathcal{C}_{i-1}} \mathcal{N}\bigRound{
            \bigCurl{
                \A{i}F_\vecbf{\bA{1, i-1}}(Z) : \A{i}\in\mathcal{B}_i 
            }, \epsilon_i, \|.\|_i
        } \\
        &\le \sup_{
            \substack{
                (\A{1}, \dots, \A{i-1}) \\
                \A{j}\in\mathcal{B}_j, j \le i-1
            }    
        } \mathcal{N}\bigRound{
            \bigCurl{
                \A{i}F_\vecbf{\A{1, i-1}}(Z) : \A{i} \in \mathcal{B}_i
            }, \epsilon_i, \|.\|_i
        }
    \end{align*}

    \noindent Now, construct $\mathcal{C}_i$ as followed:
    \begin{align*}
        \mathcal{C}_i 
            &= \bigcup_{\vecbf{\bA{1, i-1}}\in\mathcal{C}_{i-1}} \mathcal{C}_i(\vecbf{\bA{1, i-1}}) \\
        \implies
        |\mathcal{C}_i| 
            &\le \sum_{\vecbf{\bA{1, i-1}}\in\mathcal{C}_{i-1}} \bigAbs{\mathcal{C}_i(\vecbf{\bA{1, i-1}})} \\
            &\le |\mathcal{C}_{i-1}| \cdot \sup_{
                \substack{
                    (\A{1}, \dots, \A{i-1})\\
                    \A{j}\in\mathcal{B}_j, j \le i-1
                }
            } \mathcal{N}\bigRound{
                \bigCurl{
                    \A{i}F_\vecbf{\A{1, i-1}}(Z) : \A{i} \in \mathcal{B}_i
                }, \epsilon_i, \|.\|_i
            } \\
            &\le |\mathcal{C}_1|\cdot \prod_{k=2}^i \sup_{
                \substack{
                    (\A{1}, \dots, \A{k-1})\\
                    \A{j}\in\mathcal{B}_j, j \le k-1
                }
            } \mathcal{N}\bigRound{
                \bigCurl{
                    \A{k}F_\vecbf{\A{1, k-1}}(Z) : \A{k} \in \mathcal{B}_k
                }, \epsilon_k, \|.\|_k
            }
    \end{align*}

    \noindent The last inequality was achieved by expanding the expression inductively. From the construction of $\mathcal{C}_i$, for all $\vecbf{\A{1, i}}\in\mathcal{B}_1\times\dots\times\mathcal{B}_i$, we can choose $\vecbf{\bA{1, i}}\in\mathcal{C}_i$ such that:
    \begin{align*}
        \Big\|
            \A{i}F_\vecbf{\A{1, i-1}}&(Z) - \bA{i}F_\vecbf{\bA{1, i-1}}(Z)
        \Big\|_i \\
            &\le \Big\|\bigRound{\A{i} - \bA{i}}F_\vecbf{\bA{1, i-1}}(Z) \Big\|_i 
            + c_i\rho_{i-1}\Big\| \A{i-1}F_\vecbf{\A{1, i-2}}(Z) - \bA{i-1}F_\vecbf{\bA{1, i-2}}(Z) \Big\|_{i-1} \\
            &\le \epsilon_i + c_i\rho_{i-1} \bigRound{
                \epsilon_{i-1} + c_{i-1}\rho_{i-2}\Big\| \A{i-2}F_\vecbf{\A{1, i-3}}(Z) - \bA{i-2}F_\vecbf{\bA{1, i-3}}(Z) \Big\|_{i-2}
            } \\
            &\vdots \\
            &\le \sum_{j=1}^i\epsilon_j \biggRound{
                \prod_{l=j+1}^i c_l\rho_{l-1}
            }
    \end{align*} 


    \noindent By induction we have the size of the cover $\mathcal{C}_L$ satisfies the following upper bound:
    \begin{align*}
        |\mathcal{C}_L| \le |\mathcal{C}_1|\cdot\prod_{i=2}^L\sup_{
            \substack{
                (\A{1}, \dots, \A{i-1})\\
                \A{j}\in\mathcal{B}_j, j\le i-1
            }
        }\mathcal{N}\bigRound{
            \bigCurl{
                \A{i}F_\vecbf{\A{1, i-1}}(Z) : \A{i}\in\mathcal{B}_i
            }, \epsilon_i, \|.\|_i
        }
    \end{align*}

    \noindent With granularity $\tau=\sum_{j=1}^L\epsilon_j\biggRound{\prod_{l=j+1}^L c_l\rho_{l-1}}$.
\end{proof*}


\subsubsection{Solution to \ref{sec:rad_complexity_of_ramp_loss_nn} - without applying theorem \ref{thm:nn_cover_bound_with_general_norm}}
\noindent In this section, we will solve the problem in \ref{sec:rad_complexity_of_ramp_loss_nn} using the same proof technique that was used for theorem \ref{thm:nn_cover_bound_with_general_norm} without applying it directly.

\noindent\newline\textbf{1. Construct cover for $\mathcal{L}_r$ inductively}: Given a sample dataset $S=\bigCurl{(x_i, y_i)}_{i=1}^n$ and $\epsilon>0$, we need to construct an $\epsilon$-cover for $\mathcal{L}_r$ with respect to the $\|.\|_2^S$ defined as:
\begin{align*}
    \forall L_\vecbf{A} \in \mathcal{L}_r : \|L_\vecbf{A}\|_2^S = \sqrt{
        \frac{1}{n}\sum_{i=1}^n |L_\vecbf{A}(x_i, y_i)|^2
    }
\end{align*}

\noindent Where we use the notation $L_\vecbf{A}$ to specify that $L_\vecbf{A}$ is parameterized by the tuple of weight matrices $\vecbf{A}=(\A{1}, \dots, \A{L})\in\mathcal{A}$. Let $L_\vecbf{A},L_\vecbf{\bar A}\in\mathcal{L}_r$ be the loss functions for $F_\vecbf{A}, F_\vecbf{\bar A}\in\F_\mathcal{A}$, we have:
\begin{align*}
    |L_\vecbf{A}(x_i, y_i) - L_\vecbf{\bar A}(x_i, y_i)| &\le \frac{2}{r}\max_{j\in\{1, \dots, K\}} \bigAbs{
        F_\vecbf{A}(x_i)_j - F_\vecbf{\bar A}(x_i)_j
    }
\end{align*}

\noindent For each $F_\vecbf{A}\in\F_\mathcal{A}$, we define $F_\vecbf{A}$ as the entire network and $F_\vecbf{\A{1, k}}$ as the extraction of the first $k\ge 1$ layers with activation in the $k^{th}$ layer. Specifically:
\begin{align*}
    F_\vecbf{A}(x) &= \A{L}\sigma_{L-1}\bigRound{\A{L-1}\sigma_{L-2}\bigRound{ \dots \A{2}\sigma_1(\A{1}x) \dots}} \\   
    F_\vecbf{\A{1, k}}(x) &= \sigma_k\bigRound{
        \A{k}\sigma_{k-1}\bigRound{\A{k-1}\sigma_{k-2}\bigRound{\dots \A{2}\sigma_1(\A{1}x) \dots}}
    }
\end{align*}

\noindent Expanding $\max_{j\in\{1, \dots, K\}}\bigAbs{F_\vecbf{A}(x_i)_j - F_\vecbf{\bar A}(x_i)_j}$, we have:
\begin{align*}
    \max_{j\in\{1, \dots, K\}}&\bigAbs{ F_\vecbf{A}(x_i)_j - F_\vecbf{\bar A}(x_i)_j } \\
        &=  \Big\|
            \A{L}F_\vecbf{\A{1, L-1}}(x_i) - \bA{L}F_\vecbf{\bA{1, L-1}}(x_i)
        \Big\|_\infty \\
        &= \Big\|
            \bigRound{\A{L}-\bA{L}}F_\vecbf{\bA{1, L-1}}(x_i) + \A{L}\bigRound{F_\vecbf{\A{1, L-1}}(x_i) - F_\vecbf{\bA{1, L-1}}(x_i)}
        \Big\|_\infty \\
        &\le \Big\|
            \bigRound{\A{L}-\bA{L}}F_\vecbf{\bA{1, L-1}}(x_i) 
        \Big\|_\infty
        + 
        \Big\|
            \A{L}\bigRound{F_\vecbf{\A{1, L-1}}(x_i) - F_\vecbf{\bA{1, L-1}}(x_i)}
        \Big\|_\infty \\
        &\le \Big\|
            \bigRound{\A{L}-\bA{L}}F_\vecbf{\bA{1, L-1}}(x_i) 
        \Big\|_\infty
        + 
        \Big\|
            \A{L}\bigRound{F_\vecbf{\A{1, L-1}}(x_i) - F_\vecbf{\bA{1, L-1}}(x_i)}
        \Big\|_2 \\
        &\le \max_{x_*\in S}\Big\|
            \bigRound{\A{L}-\bA{L}}F_\vecbf{\bA{1, L-1}}(x_*)
        \Big\|_\infty + \|\A{L}\|_\sigma\Big\|
            F_\vecbf{\A{1, L-1}}(x_i) - F_\vecbf{\bA{1, L-1}}(x_i)
        \Big\|_2 \\
        &\le \max_{x_*\in S}\Big\|
            \bigRound{\A{L}-\bA{L}}F_\vecbf{\bA{1, L-1}}(x_*)
        \Big\|_\infty + s_L \Big\|
            F_\vecbf{\A{1, L-1}}(x_i) - F_\vecbf{\bA{1, L-1}}(x_i)
        \Big\|_2 \\
        &\le \max_{x_*\in S}\Big\|
            \bigRound{\A{L}-\bA{L}}F_\vecbf{\bA{1, L-1}}(x_*)
        \Big\|_\infty + s_L\rho_{L-1} \Big\|
            \A{L-1}F_\vecbf{\A{1, L-2}}(x_i) - \bA{L-1}F_\vecbf{\bA{1, L-2}}(x_i) 
        \Big\|_2 \\
\end{align*}

\noindent\newline\textbf{1.1. Construct cover for the first $L-1$ layers}: For $1\le k \le L-1$, construct the cover $\mathcal{C}_k$ with respect to the $\|.\|_2^S$ for the following class:
\begin{align*}
    \F_k = \bigCurl{
        x\mapsto \A{k}F_\vecbf{\A{1, k-1}}(x) : \|\A{k}\|_\sigma \le s_k, \ \|(\A{k} - \M{k})^T\|_{2, 1}\le a_i 
    }
\end{align*}

\noindent For every $x_i\in S$, we have:
\begin{align*}
    \Big\|\A{k}F_\vecbf{\A{1, k-1}}(x_i) &- \bA{k}F_\vecbf{\bA{1, k-1}}(x_i)\Big\|_2 \\
        &= \Big\|
            \bigRound{\A{k} - \bA{k}}F_\vecbf{\bA{1, k-1}}(x_i)
        \Big\|_2 + \Big\|
            \A{k}\bigRound{F_\vecbf{\A{1, k-1}}(x_i) - F_\vecbf{\bA{1, k-1}}(x_i)}
        \Big\|_2 \\
        &= \Big\|
            \bigRound{\A{k} - \bA{k}}F_\vecbf{\bA{1, k-1}}(x_i)
        \Big\|_2 + s_k\rho_{k-1}\Big\|
            \A{k-1}\bigRound{F_\vecbf{\A{1, k-2}}(x_i) - F_\vecbf{\bA{1, k-2}}(x_i)}
        \Big\|_2 \\
\end{align*}

\noindent Expand the above inductively in a similar manner as theorem \ref{thm:nn_cover_bound_with_general_norm} and for each layer index $1\le j\le k$, denote $\mathcal{B}_l=\bigCurl{\A{l}\in\R^{d_{l-1}\times d_l} : \|\A{l}\|_\sigma\le s_l, \|(\A{l}-\M{l})^T\|_{2, 1} \le a_l}$ and $\mathcal{C}_1$ is the cover of the first layer, we have:
\begin{align*}
    |\mathcal{C}_k| \le |\mathcal{C}_1| \cdot \prod_{j=2}^k \sup_{ 
        \substack{
            (\A{1}, \dots, \A{j-1}) \\
            \forall l \le j-1:\A{l}\in\mathcal{B}_l
        } 
    } \mathcal{N}\bigRound{
        \bigCurl{
            x\mapsto \A{j}F_\vecbf{\A{1, j-1}}(x) : \A{j}\in\mathcal{B}_j
        }, \epsilon_j, \|.\|_2^S 
    }
\end{align*}

\noindent Where $\epsilon_1, \dots, \epsilon_k$ are known positive constants. Denote $F_\vecbf{\A{1, j-1}}(x)=x$ for $j=1$ and also denote $W=\max_{0\le l \le L-1}d_l$. By lemma 3.2 from \cite{article:bartlett}, we have:
\begin{align*}
    \log|\mathcal{C}_k| 
    &\le \sum_{j=1}^k\sup_{
        \substack{
            (\A{1}, \dots, \A{j-1}) \\ \forall l \le j-1: \A{l}\in\mathcal{B}_l
        }
    }\log\mathcal{N}\bigRound{
        \bigCurl{
            x \mapsto \A{j}F_\vecbf{\A{1, j-1}}(x) : \A{j} \in \mathcal{B}_j
        }, \epsilon_j, \|.\|_2^S
    } \\
    &\le \sum_{j=1}^k\sup_{
        \substack{
            (\A{1}, \dots, \A{j-1}) \\ \forall l \le j-1: \A{l}\in\mathcal{B}_l, \|x\|_2 \le 1
        }
    }\frac{a_j^2\|F_\vecbf{\A{1, j-1}}(x)\|_2^2}{\epsilon_j^2}\log(2d_{j-1}d_j) \\
    &\le \sum_{j=1}^k\sup_{
        \substack{
            (\A{1}, \dots, \A{j-1}) \\ \forall l \le j-1: \A{l}\in\mathcal{B}_l, \|x\|_2 \le 1
        }
    }\frac{a_j^2\|F_\vecbf{\A{1, j-1}}(x)\|_2^2}{\epsilon_j^2}\log(2W^2) \\
\end{align*}

\noindent To bound the output at the $(j-1)^{th}$ layer, we have:
\begin{align*}
    \Big\|
        F_\vecbf{\A{1, j-1}}(x)
    \Big\|_2 &= \Big\|
        \sigma_{j-1}\bigRound{\A{j-1}\sigma_{j-2}\bigRound{\dots \A{2}\sigma_1(\A{1}x) \dots}}
    \Big\|_2 \\
    &\le \rho_{j-1}s_{j-1} \Big\|
        \sigma_{j-2}\bigRound{\A{j-2}\sigma_{j-3}\bigRound{\dots \A{2}\sigma_1(\A{1}x) \dots}}
    \Big\| \\
    &\le \prod_{l=1}^{j-1}\rho_l s_l
\end{align*}

\noindent Hence, we have:
\begin{align*}
    \log|\mathcal{C}_k| \le \sum_{j=1}^k \frac{a_j^2\prod_{1\le l < j-1} \rho_l^2s_l^2}{\epsilon_j^2}\log(2W^2)
\end{align*}

\noindent Furthermore:
\begin{align*}
    \Big\|
        \A{k}F_\vecbf{\A{1, k-1}(x_i)} &- \bA{k}F_\vecbf{\bA{1,k-1}(x_i)} 
    \Big\|_2 \\ &\le \Big\|
        \bigRound{\A{k} - \bA{k}}F_\vecbf{\bA{1, k-1}}(x_i)
    \Big\|_2 + s_k\rho_{k-1}\Big\|
        \A{k-1}\bigRound{
            F_\vecbf{\A{1, k-2}}(x_i) - F_\vecbf{\bA{1, k-2}}(x_i)
        }
    \Big\|_2 \\ 
    &\le \epsilon_k + s_k\rho_{k-1}\bigRound{\epsilon_{k-1} + s_{k-1}\rho_{k-2}\Big\|
        \A{k-2}\bigRound{
            F_\vecbf{\A{1, k-3}}(x_i) - F_\vecbf{\bA{1, k-3}}(x_i)
        }
    \Big\|_2} \\
    &\vdots \\
    &\le \sum_{j=1}^k\epsilon_j \biggRound{
        \prod_{l=j+1}^{k} s_{l}\rho_{l-1}
    }
\end{align*}

\noindent Then, by induction, we can derive the size of the cover $\mathcal{C}_{L-1}$:
\begin{align*}
    \log|\mathcal{C}_{L-1}| \le \sum_{j=1}^{L-1}\frac{a_j^2\prod_{1\le l \le j-1}\rho_l^2s_l^2}{\epsilon_j^2}\log(2W^2) \ \ \ (1)
\end{align*}

\noindent With the aggregated granularity:
\begin{align*}
    \tau_{L-1} = \sum_{j=1}^{L-1}\epsilon_j \biggRound{\prod_{l=j+1}^{L-1} s_l\rho_{l-1}}
\end{align*}


\noindent\newline\textbf{1.2. Construct cover for $L$ layers}: Let $\epsilon_L>0$ be a known constant. We define the $\|.\|_*^S$ norm for a function $f$ as followed:
\begin{align*}
    \|f\|_*^S = \max_{x_* \in S}\|f(x_*)\|_\infty
\end{align*} 


\noindent For each element $\vecbf{\bA{1, L-1}}\in\mathcal{C}_{L-1}$, construct the $\epsilon_L$-cover $\mathcal{C}_L(\vecbf{\bA{1, L-1}})$ with respect to the $\|.\|_*^S$ norm for the following class:
\begin{align*}
    \F_L(\vecbf{\bA{1, L-1}}) = \biggCurl{
        x \mapsto \A{L}F_\vecbf{\bA{1, L-1}}(x) : \|\A{L}-\M{L}\|_F \le a_*
    }
\end{align*} 

\noindent Now, construct the cover $\mathcal{C}_L$ as followed:
\begin{align*}
    \mathcal{C}_L &= \bigcup_{\vecbf{\bA{1, L-1}}\in\mathcal{C}_{L-1}} \mathcal{C}_L\bigRound{
        \vecbf{\bA{1, L-1}}
    } \\
    \implies 
    |\mathcal{C}_L| &\le \sum_{\vecbf{\bA{1, L-1}}\in\mathcal{C}_{L-1}} \mathcal{C}_L\bigRound{
        \vecbf{\bA{1, L-1}}
    } \\
    &\le |\mathcal{C}_{L-1}| \cdot \sup_{\vecbf{\bA{1, L-1}}\in\mathcal{C}_{L-1}}\bigAbs{
        \mathcal{C}_L\bigRound{
            \vecbf{\bA{1, L-1}}
        }
    } \\
    &\le |\mathcal{C}_{L-1}| \cdot \sup_{
        \substack{
            (\A{1}, \dots, \A{L-1}) \\
            \forall 1\le j\le L-1 : \A{j}\in\mathcal{B}_j
        }
    }\mathcal{N}\bigRound{
        \bigCurl{
            x \mapsto \A{L}F_\vecbf{\A{1, L-1}}(x) : \|\A{L} - \M{L}\|_F \le a_*
        }, \epsilon_L, \|.\|_*^S
    } \ \ \ (2)
\end{align*}

\noindent We have:
\begin{align*}
    \sup_{\substack{
        (\A{1}, \dots, \A{L-1}) \\
        \forall 1 \le j \le L-1 : \A{j}\in\mathcal{B}_j
    }} \Big\|
        F_\vecbf{\A{1, L-1}}(x)
    \Big\|_2 \le \prod_{l=1}^{L-1} \rho_ls_l
\end{align*}

\noindent By proposition 5 from \cite{article:ledent_and_mustafa}, we have:
\begin{align*}
    \log\mathcal{N}\bigRound{
        \bigCurl{
            x\mapsto \A{L}F_\vecbf{\A{1, L-1}}(x) &: \|\A{L} - \M{L}\|_F \le a_*
        }, \epsilon_L, \|.\|_*^S
    } \\
    &\le \frac{36a_*^2\prod_{1\le l \le L-1}\rho_l^2s_l^2}{\epsilon_L^2}\log\biggRound{
        \biggRound{
            \frac{8a_*\prod_{1\le l \le L-1}\rho_ls_l}{\epsilon_L} + 7
        }nd_L
    } \\
    &\le \frac{36a_*^2\prod_{1\le l \le L-1}\rho_ls_l}{\epsilon_L^2}\log\biggRound{
        \biggRound{
            \frac{8a_*\prod_{1\le l \le L-1}\rho_ls_l}{\epsilon_L} + 7
        }nW
    } \ \ \ (3)
\end{align*}

\noindent\newline\textbf{1.3. Combine the covers}: Let $a_L = 6a_*$, from $(1), (2)$ and $(3)$, we have
\begin{align*}
    \log|\mathcal{C}_L| 
    &\le \log|\mathcal{C}_{L-1}| + \frac{36a_*^2\prod_{l=1}^{L-1}\rho_l^2s_l^2}{\epsilon_L^2}\log\biggRound{
        \biggRound{
            \frac{8a_*\prod_{l=1}^{L-1}\rho_ls_l}{\epsilon_L} + 7
        }nW
    }
    \\
    &\le \log(2W^2)\sum_{j=1}^{L-1}\frac{a_j^2\prod_{l=1}^{j-1}\rho_l^2s_l^2}{\epsilon_j^2} + \frac{a_L^2\prod_{l=1}^{L-1}\rho_l^2s_l^2}{\epsilon_L^2}\log\biggRound{
        \biggRound{
            \frac{4a_L\prod_{l=1}^{L-1}\rho_ls_l}{3\epsilon_L} + 7
        }nW
    } \\
    &\le \log(\mathcal{D}) \sum_{j=1}^L \frac{a_j^2\prod_{l=1}^{j-1}\rho_l^2s_l^2}{\epsilon_j^2}
\end{align*}

\noindent Where $\mathcal{D}$ is a constant that depends on the sample size and the maximum width of the network, defined as followed:
\begin{align*}
    \mathcal{D} = W\max\biggCurl{
        2W, \biggRound{
            \frac{4a_L\prod_{l=1}^{L-1}\rho_ls_l}{3\epsilon_L}
        }n
    }
\end{align*}

\noindent Now, we need to check the aggregated granularity for the cover $\mathcal{C}_L$. We have: For all $F_\vecbf{A}\in\F_\mathcal{A}$, there exists $\vecbf{\bar A}\in\mathcal{C}_L$ such that:
\begin{align*}
    \max_{j\in\{1, \dots, K\}} &\bigAbs{
        F_\vecbf{A}(x_i)_j - F_\vecbf{\bar A}(x_i)_j
    } \\
    &\le \max_{x_*\in S}\Big\|
        \bigRound{\A{L}-\bA{L}}F_\vecbf{\bA{1, L-1}}(x_*)
    \Big\|_\infty + s_L\rho_{L-1} \Big\|
        \A{L-1}F_\vecbf{\A{1, L-2}}(x_i) - \bA{L-1}F_\vecbf{\bA{1, L-2}}(x_i) 
    \Big\|_2 \\ 
    &\le \epsilon_L + s_L\rho_{L-1}\biggRound{
        \sum_{j=1}^{L-1}\epsilon_j\biggRound{
            \prod_{l=j+1}^{L-1}s_l\rho_{l-1}
        }
    } \\
    &=\sum_{j=1}^L \epsilon_j\biggRound{\prod_{l=j+1}^L s_l\rho_{l-1}}
\end{align*}

\noindent For a given granularity $\bar \epsilon > 0$ that we want to impose on $\mathcal{C}_L$, we set each $\epsilon_j, \ 1 \le j \le L$ similar to \cite{article:bartlett} as followed:
\begin{align*}
    \epsilon_j = \frac{\alpha_j\bar\epsilon}{\prod_{l=j+1}^L s_l\rho_{l-1}}, \text{where : } \alpha_j = \frac{1}{\bar\alpha}\biggRound{\frac{a_j}{s_j}}^{2/3}, \ \bar\alpha=\sum_{l=1}^L \biggRound{\frac{a_l}{s_l}}^{2/3}
\end{align*}

\noindent Hence, we have:
\begin{align*}
    \sum_{j=1}^L \epsilon_j\biggRound{
        \prod_{l=j+1}^L s_l\rho_{l-1}
    } &= \sum_{j=1}^L \frac{\bar\epsilon(a_j/s_j)^{2/3}\prod_{l=j+1}^L s_l\rho_{l-1}}{\bar\alpha\prod_{l=j+1}^Ls_l\rho_{l-1}} \\
    &= \sum_{j=1}^L\alpha_j\bar\epsilon = \bar\epsilon
\end{align*}

\noindent Plugging the formulas for $\epsilon_j, \ 1\le j \le L$ back into the covering number bounds, we have:
\begin{align*}
    \log|\mathcal{C}_L| &\le \log(\mathcal{D})\sum_{j=1}^L \frac{a_j^2\prod_{l=1}^{j-1}\rho_l^2s_l^2\prod_{k=j+1}^L s_k^2\rho_{k-1}^2}{\alpha_j^2\bar\epsilon^2} \\
        &= \log(\mathcal{D})\sum_{j=1}^L \frac{a_j^2\prod_{l=1}^L \rho_l^2s_l^2}{s_j^2\alpha_j^2\bar\epsilon^2}, \ \ \ (\text{Assuming }\rho_L=1) \\
        &= \log(\mathcal{D})\frac{\prod_{l=1}^L \rho_l^2s_l^2}{\bar\epsilon^2}\sum_{j=1}^L \frac{a_j^2}{s_j^2\alpha_j^2} \\
        &= \log(\mathcal{D})\frac{\prod_{l=1}^L \rho_l^2s_l^2}{\bar\epsilon^2}\sum_{j=1}^L \frac{a_j^2\bar\alpha^2}{s_j^2(a_j/s_j)^{4/3}} \\
        &= \log(\mathcal{D})\frac{\prod_{l=1}^L \rho_l^2s_l^2}{\bar\epsilon^2}\bar\alpha^2\sum_{j=1}^L \biggRound{\frac{a_j}{s_j}}^{2/3} = \bar\alpha^3\log(\mathcal{D})\frac{\prod_{l=1}^L\rho_l^2s_l^2}{\bar\epsilon^2}
\end{align*}

\noindent\newline\textbf{1.4. Covering the loss function class}: For a given granularity $\epsilon>0$ that we want to impose on the class of loss functions $\mathcal{L}_r$, set $\bar\epsilon=r\epsilon/2$, we have:
\begin{align*}
    \log\mathcal{N}\bigRound{
        \mathcal{L}_r, \epsilon, \|.\|_2^S
    } &\le 4\bar\alpha^3\log(\mathcal{D})\frac{\prod_{l=1}^L\rho_l^2s_l^2}{r^2\epsilon^2} = \frac{4\mathcal{R_A}\log(\mathcal{D})}{r^2\epsilon^2}
\end{align*}

\noindent Where we have:
\begin{align*}
    \mathcal{R_A} = \prod_{l=1}^L\rho_l^2s_l^2 \biggRound{
        \sum_{j=1}^L \biggRound{
            \frac{a_l}{s_l}
        }^{2/3}
    }^3, \ \ \ \bigRound{a_L = 6a_*}
\end{align*}

\noindent And, plugging $\epsilon_L=r\epsilon/(2\bar\alpha)$ into the formula of $\mathcal{D}$, we have:
\begin{align*}
    \mathcal{D} = W\max\biggRound{
        2W, \biggRound{
            \frac{8\bar\alpha a_L\prod_{l=1}^{L-1}\rho_ls_l}{3r\epsilon}
        }n
    }
\end{align*}

