\newpage
\subsection{Rademacher Complexity of ramp loss for neural networks}
\subsubsection{Problem Statement}
\label{sec:rad_complexity_of_ramp_loss_nn}
\textbf{Problem} : Given a tuple $\begin{pmatrix}\M{1} & \dots & \M{L}\end{pmatrix}$ of $L\ge 1$ reference matrices (that represents initial weights in a neural network). Let $\mathcal{A}$ be the class of tuples of $L$ weight matrices defined as followed:
\begin{align*}
    \mathcal{A} = \bigCurl{
        \begin{pmatrix}
            \A{1} & \dots & \A{L}
        \end{pmatrix} : \|\A{i}\|_\sigma \le s_i, \|(\A{i} - \M{i})^T\|_{2, 1}\le a_i, \|\A{L} - \M{L}\|_F \le a_*
    }
\end{align*}

\noindent Where $\|.\|_\sigma$ denotes the spectral norm and $\|.\|_{p, q}$ denotes the matrix $(p, q)$ norm defined as $\|A\|_{p, q} = \bigRound{\sum_j\bigRound{\sum_{i}|A_{ij}|^p}^{q/p}}^{1/q}$. Therefore, the $\|.\|_{2, 1}$ is defined as:
\begin{align*}
    \|A\|_{2, 1} = \sum_{j=1}^m \biggRound{
        \sum_{i=1}^n |A_{ij}|^2
    }^{1/2}
    = \sum_{j=1}^m \|A_{., j}\|_2, \ \ A\in\R^{m\times n}
\end{align*}

\noindent Define the following class of neural networks:
\begin{align*}
    \F_\mathcal{A} = \biggCurl{
        x\mapsto \A{L} \biggRound{
            \bigO_{k=1}^{L-1} \sigma_k \circ \A{k}
        }(x) : \sigma_k \text{ is } \rho_k\text{-Lipchitz w.r.t $l_2$ norm}, \begin{pmatrix}
            \A{1} & \dots & \A{L}
        \end{pmatrix} \in \mathcal{A}
    }
\end{align*}

\noindent We denote $d_0, d_1, \dots, d_L$ as the widths of each layer of the neural networks where $d_0$ denotes the dimensionality of the input and $d_L=K$ be the size of the output. 


\noindent Derive the bound for the Rademacher Complexity of the loss function class:
\begin{align*}
    \mathcal{L}_r = \biggCurl{
        (x, y) \mapsto l_r\bigRound{
            F_\vecbf{A}(x)_y - \max_{k\ne y}F_\vecbf{A}(x)_k
        } \Bigg| F_\vecbf{A} \in \F_\mathcal{A}
    }
\end{align*}

\noindent Where $l_r$ is the ramp loss function with margin $r\in(0,1)$.

\subsubsection{Neural networks covering bounds with general norm}
\begin{theorem}{Neural networks covering bound with general norm}{nn_cover_bound_with_general_norm}
    Let $L \ge 1$ be a natural number and $\epsilon_1, \dots, \epsilon_L > 0$ be given as covering number granularities. Given the following:
    \begin{itemize}
        \item A sequence of vector spaces $\mathcal{V}_0, \dots, \mathcal{V}_L$ endowed with norms $|.|_0, \dots, |.|_L$.
        \item A sequence of vector spaces $\mathcal{W}_1, \dots, \mathcal{W}_L$ endowed with norms $\|.\|_1, \dots, \|.\|_L$.
        \item A sequence of real positive numbers $c_1, \dots, c_L$ and linear operators $\A{i} : \mathcal{V}_i \to \mathcal{W}_{i+1}$ associated with the operator norm:
        \begin{align*}
            \|\A{i}\|_{op} = \sup_{|Z|_i \le 1} \|A_iZ\|_{i+1} \le c_i, \ \forall i \in \{1, \dots, L\}
        \end{align*}

        \item A sequence of real positive numbers $\rho_1, \dots, \rho_L$ and activation functions $\sigma_i:\mathcal{W}_i \to \mathcal{V}_i$ such that $\sigma_i$ are $\rho_i$-Lipchitz:
        \begin{align*}
            |\sigma_i(z_1) - \sigma_i(z_2)|_i \le \rho_i\|z_1 - z_2\|_i
        \end{align*}

        \item Let $\mathcal{A}\subseteq \mathcal{B}_1\times\dots\times\mathcal{B}_L$ be a class of tuples of matrices $(\A{1}, \dots, \A{L})$ such that each $\A{i} \in \mathcal{B}_i$ satisfies $\|\A{i}\|_{op} \le c_i$.

        \item Define the class of neural networks $\F_\mathcal{A}$ as followed:
        \begin{align*}
            \F_\mathcal{A} = \biggCurl{
                x\mapsto \A{L}\biggRound{
                    \bigO_{k=1}^{L-1}\sigma_k \circ \A{k}
                }(x) : (\A{1}, \dots, \A{L}) \in \mathcal{A}
            }
        \end{align*}
        
        \item Let $\tau$ be the aggregated granularity defined as $\tau = \sum_{j=1}^L \epsilon_j \bigRound{\prod_{l=j+1}^L c_l\rho_{l-1}}$ and $Z\subset\mathcal{V}_0$ be a sample dataset. We have:
        \begin{align*}
            \mathcal{N}\bigRound{
                {\F_\mathcal{A}}_{|Z}, \tau, \|.\|_L
            } \le |\mathcal{C}_1|\cdot\prod_{i=2}^L \sup_{\substack{
                (\A{1}, \dots, \A{i-1}) \\
                \A{j} \in \mathcal{B}_j, j \le i-1
            }}\mathcal{N}\bigRound{
                \bigCurl{\A{i}F_\vecbf{\A{1, i-1}}(Z) : \A{i} \in \mathcal{B}_i}, \epsilon_i, \|.\|_i
            }
        \end{align*}
    \end{itemize}
\end{theorem}

\begin{proof*}[Theorem \ref{thm:nn_cover_bound_with_general_norm}]
    \noindent We will prove the above theorem inductively. 
    
    \noindent\newline\textbf{Base case} : Suppose we have a sample dataset $Z\subset\mathcal{V}_0$. Construct a minimum $\epsilon_1$-cover for the weight matrices in the $1^{st}$ layer called $\mathcal{C}_1$ such that for all $\A{1}\in\mathcal{B}_i$, there exists $\bA{1}\in\mathcal{C}_1$ such that:

    \begin{align*}
        \Big\|\A{1}Z - \bA{1}Z\Big\|_1 \le \epsilon_1
    \end{align*} 
    
    
    \noindent \textbf{Inductive step} : For $i, j \ge 2, j \ge i$, define $\vecbf{\A{i, j}}\in\mathcal{B}_i\times\dots\times\mathcal{B}_j$ as the extraction of layers $i$ to $j$ for every $\vecbf{A}\in\mathcal{A}$. Hence, define the function $F_\vecbf{\A{i, j}}$ as followed:
    \begin{align*}
        F_\vecbf{\A{i, j}}(Z) = \sigma_j\bigRound{
            \A{j}\sigma_{j-1}\bigRound{
                \A{j-1}\dots\sigma_i\bigRound{\A{i}Z}\dots
            }
        }
    \end{align*} 
    
    
    \noindent We need to construct a cover $\mathcal{C}_i$ with respect to norm $\|.\|_i$ with granularity $\tau_i>0$ such that for all $\vecbf{\A{1, i}}\in \mathcal{B}_1\times\dots\times\mathcal{B}_i$, there exists $\vecbf{\bA{1, i}}\in \mathcal{C}_{i}$ such that: 
    \begin{align*}
        \Big\|
            \A{i}F_\vecbf{\A{1, i-1}}(Z) - \bA{i}F_\vecbf{\bA{1, i-1}}(Z)
        \Big\|_i \le \tau_i
    \end{align*}

    \noindent Suppose that we have constructed covers $\mathcal{C}_{i-1}, \mathcal{C}_{i-2}, \dots, \mathcal{C}_1$ with respect to norms $\|.\|_{i-1}, \|.\|_{i-2}, \dots, \|.\|_1$ with granularities $\tau_{i-1}, \tau_{i-2}, \dots, \tau_1$ (where $\mathcal{C}_1$ is the cover constructed in the base case with \newline granularity $\tau_1=\epsilon_1$). We have:

    \begin{align*}
        \Big\|
            \A{i}F_\vecbf{\A{1, i-1}}&(Z) - \bA{i}F_\vecbf{\bA{1, i-1}}(Z)
        \Big\|_i \\
            &\le \Big\|\bigRound{\A{i} - \bA{i}}F_\vecbf{\bA{1, i-1}}(Z) \Big\|_i 
                + \Big\| \A{i}\bigRound{F_\vecbf{\A{1, i-1}}(Z) - F_\vecbf{\bA{1, i-1}}(Z)}\Big\|_i \\
            &\le \Big\|\bigRound{\A{i} - \bA{i}}F_\vecbf{\bA{1, i-1}}(Z) \Big\|_i 
                + \Big\|\A{i}\Big\|_{op} \cdot\Big| F_\vecbf{\A{1, i-1}}(Z) - F_\vecbf{\bA{1, i-1}}(Z) \Big|_{i-1} \\
            &\le \Big\|\bigRound{\A{i} - \bA{i}}F_\vecbf{\bA{1, i-1}}(Z) \Big\|_i 
                + c_i\Big| F_\vecbf{\A{1, i-1}}(Z) - F_\vecbf{\bA{1, i-1}}(Z) \Big|_{i-1} \\
            &\le \Big\|\bigRound{\A{i} - \bA{i}}F_\vecbf{\bA{1, i-1}}(Z) \Big\|_i 
                + c_i\rho_{i-1}\Big\| \A{i-1}F_\vecbf{\A{1, i-2}}(Z) - \bA{i-1}F_\vecbf{\bA{1, i-2}}(Z) \Big\|_{i-1} 
    \end{align*}

    \noindent\newline For each $\vecbf{\bA{1, i-1}}\in \mathcal{C}_{i-1}$, construct the minimum $\epsilon_i$-cover $\mathcal{C}_i(\vecbf{\bA{1, i-1}})$ for the class \newline$\bigCurl{\A{i}F_\vecbf{\bA{1, i-1}}(Z) : \A{i} \in \mathcal{B}_i}$. Then, we have:
    \begin{align*}
        \bigAbs{
            \mathcal{C}_i(\vecbf{\bA{1, i-1}})
        } &\le \sup_{\vecbf{\bA{1, i-1}}\in\mathcal{C}_{i-1}} \mathcal{N}\bigRound{
            \bigCurl{
                \A{i}F_\vecbf{\bA{1, i-1}}(Z) : \A{i}\in\mathcal{B}_i 
            }, \epsilon_i, \|.\|_i
        } \\
        &\le \sup_{
            \substack{
                (\A{1}, \dots, \A{i-1}) \\
                \A{j}\in\mathcal{B}_j, j \le i-1
            }    
        } \mathcal{N}\bigRound{
            \bigCurl{
                \A{i}F_\vecbf{\A{1, i-1}}(Z) : \A{i} \in \mathcal{B}_i
            }, \epsilon_i, \|.\|_i
        }
    \end{align*}

    \noindent Now, construct $\mathcal{C}_i$ as followed:
    \begin{align*}
        \mathcal{C}_i 
            &= \bigcup_{\vecbf{\bA{1, i-1}}\in\mathcal{C}_{i-1}} \mathcal{C}_i(\vecbf{\bA{1, i-1}}) \\
        \implies
        |\mathcal{C}_i| 
            &\le \sum_{\vecbf{\bA{1, i-1}}\in\mathcal{C}_{i-1}} \bigAbs{\mathcal{C}_i(\vecbf{\bA{1, i-1}})} \\
            &\le |\mathcal{C}_{i-1}| \cdot \sup_{
                \substack{
                    (\A{1}, \dots, \A{i-1})\\
                    \A{j}\in\mathcal{B}_j, j \le i-1
                }
            } \mathcal{N}\bigRound{
                \bigCurl{
                    \A{i}F_\vecbf{\A{1, i-1}}(Z) : \A{i} \in \mathcal{B}_i
                }, \epsilon_i, \|.\|_i
            } \\
            &\le |\mathcal{C}_1|\cdot \prod_{k=2}^i \sup_{
                \substack{
                    (\A{1}, \dots, \A{k-1})\\
                    \A{j}\in\mathcal{B}_j, j \le k-1
                }
            } \mathcal{N}\bigRound{
                \bigCurl{
                    \A{k}F_\vecbf{\A{1, k-1}}(Z) : \A{k} \in \mathcal{B}_k
                }, \epsilon_k, \|.\|_k
            }
    \end{align*}

    \noindent The last inequality was achieved by expanding the expression inductively. From the construction of $\mathcal{C}_i$, for all $\vecbf{\A{1, i}}\in\mathcal{B}_1\times\dots\times\mathcal{B}_i$, we can choose $\vecbf{\bA{1, i}}\in\mathcal{C}_i$ such that:
    \begin{align*}
        \Big\|
            \A{i}F_\vecbf{\A{1, i-1}}&(Z) - \bA{i}F_\vecbf{\bA{1, i-1}}(Z)
        \Big\|_i \\
            &\le \Big\|\bigRound{\A{i} - \bA{i}}F_\vecbf{\bA{1, i-1}}(Z) \Big\|_i 
            + c_i\rho_{i-1}\Big\| \A{i-1}F_\vecbf{\A{1, i-2}}(Z) - \bA{i-1}F_\vecbf{\bA{1, i-2}}(Z) \Big\|_{i-1} \\
            &\le \epsilon_i + c_i\rho_{i-1} \bigRound{
                \epsilon_{i-1} + c_{i-1}\rho_{i-2}\Big\| \A{i-2}F_\vecbf{\A{1, i-3}}(Z) - \bA{i-2}F_\vecbf{\bA{1, i-3}}(Z) \Big\|_{i-2}
            } \\
            &\vdots \\
            &\le \sum_{j=1}^i\epsilon_j \biggRound{
                \prod_{l=j+1}^i c_l\rho_{l-1}
            }
    \end{align*} 


    \noindent By induction we have the size of the cover $\mathcal{C}_L$ satisfies the following upper bound:
    \begin{align*}
        |\mathcal{C}_L| \le |\mathcal{C}_1|\cdot\prod_{i=2}^L\sup_{
            \substack{
                (\A{1}, \dots, \A{i-1})\\
                \A{j}\in\mathcal{B}_j, j\le i-1
            }
        }\mathcal{N}\bigRound{
            \bigCurl{
                \A{i}F_\vecbf{\A{1, i-1}}(Z) : \A{i}\in\mathcal{B}_i
            }, \epsilon_i, \|.\|_i
        }
    \end{align*}

    \noindent With granularity $\tau=\sum_{j=1}^L\epsilon_j\biggRound{\prod_{l=j+1}^L c_l\rho_{l-1}}$.
\end{proof*}


\subsubsection{Solution to \ref{sec:rad_complexity_of_ramp_loss_nn} - without applying theorem \ref{thm:nn_cover_bound_with_general_norm}}
\noindent In this section, we will solve the problem in \ref{sec:rad_complexity_of_ramp_loss_nn} using the same proof technique that was used for theorem \ref{thm:nn_cover_bound_with_general_norm} without applying it directly.

\noindent\newline\textbf{1. Construct cover for $\mathcal{L}_r$ inductively}: Given a sample dataset $S=\bigCurl{(x_i, y_i)}_{i=1}^n$ and $\epsilon>0$, we need to construct an $\epsilon$-cover for $\mathcal{L}_r$ with respect to the $\|.\|_2^S$ defined as:
\begin{align*}
    \forall L_\vecbf{A} \in \mathcal{L}_r : \|L_\vecbf{A}\|_2^S = \sqrt{
        \frac{1}{n}\sum_{i=1}^n |L_\vecbf{A}(x_i, y_i)|^2
    }
\end{align*}

\noindent Where we use the notation $L_\vecbf{A}$ to specify that $L_\vecbf{A}$ is parameterized by the tuple of weight matrices $\vecbf{A}=(\A{1}, \dots, \A{L})\in\mathcal{A}$. Let $L_\vecbf{A},L_\vecbf{\bar A}\in\mathcal{L}_r$ be the loss functions for $F_\vecbf{A}, F_\vecbf{\bar A}\in\F_\mathcal{A}$, we have:
\begin{align*}
    |L_\vecbf{A}(x_i, y_i) - L_\vecbf{\bar A}(x_i, y_i)| &\le \frac{2}{r}\max_{j\in\{1, \dots, K\}} \bigAbs{
        F_\vecbf{A}(x_i)_j - F_\vecbf{\bar A}(x_i)_j
    }
\end{align*}

\noindent For each $F_\vecbf{A}\in\F_\mathcal{A}$, we define $F_\vecbf{A}$ as the entire network and $F_\vecbf{\A{1, k}}$ as the extraction of the first $k\ge 1$ layers with activation in the $k^{th}$ layer. Specifically:
\begin{align*}
    F_\vecbf{A}(x) &= \A{L}\sigma_{L-1}\bigRound{\A{L-1}\sigma_{L-2}\bigRound{ \dots \A{2}\sigma_1(x) \dots}} \\   
    F_\vecbf{\A{1, k}}(x) &= \sigma_k\bigRound{
        \A{k}\sigma_{k-1}\bigRound{\A{k-1}\sigma_{k-2}\bigRound{\dots \A{2}\sigma_1(x) \dots}}
    }
\end{align*}

\noindent Expanding $\max_{j\in\{1, \dots, K\}}\bigAbs{F_\vecbf{A}(x_i)_j - F_\vecbf{\bar A}(x_i)_j}$, we have:
\begin{align*}
    \max_{j\in\{1, \dots, K\}}&\bigAbs{ F_\vecbf{A}(x_i)_j - F_\vecbf{\bar A}(x_i)_j } \\
        &=  \Big\|
            \A{L}F_\vecbf{\A{1, L-1}}(x_i) - \bA{L}F_\vecbf{\bA{1, L-1}}(x_i)
        \Big\|_\infty \\
        &= \Big\|
            \bigRound{\A{L}-\bA{L}}F_\vecbf{\bA{1, L-1}}(x_i) + \A{L}\bigRound{F_\vecbf{\A{1, L-1}}(x_i) - F_\vecbf{\bA{1, L-1}}(x_i)}
        \Big\|_\infty \\
        &\le \Big\|
            \bigRound{\A{L}-\bA{L}}F_\vecbf{\bA{1, L-1}}(x_i) 
        \Big\|_\infty
        + 
        \Big\|
            \A{L}\bigRound{F_\vecbf{\A{1, L-1}}(x_i) - F_\vecbf{\bA{1, L-1}}(x_i)}
        \Big\|_\infty \\
        &\le \Big\|
            \bigRound{\A{L}-\bA{L}}F_\vecbf{\bA{1, L-1}}(x_i) 
        \Big\|_\infty
        + 
        \Big\|
            \A{L}\bigRound{F_\vecbf{\A{1, L-1}}(x_i) - F_\vecbf{\bA{1, L-1}}(x_i)}
        \Big\|_2 \\
        &\le \max_{x_*\in S}\Big\|
            \bigRound{\A{L}-\bA{L}}F_\vecbf{\bA{1, L-1}}(x_*)
        \Big\|_\infty + \|\A{L}\|_\sigma\Big\|
            F_\vecbf{\A{1, L-1}}(x_i) - F_\vecbf{\bA{1, L-1}}(x_i)
        \Big\|_2 \\
        &\le \max_{x_*\in S}\Big\|
            \bigRound{\A{L}-\bA{L}}F_\vecbf{\bA{1, L-1}}(x_*)
        \Big\|_\infty + s_L \Big\|
            F_\vecbf{\A{1, L-1}}(x_i) - F_\vecbf{\bA{1, L-1}}(x_i)
        \Big\|_2 \\
        &\le \max_{x_*\in S}\Big\|
            \bigRound{\A{L}-\bA{L}}F_\vecbf{\bA{1, L-1}}(x_*)
        \Big\|_\infty + s_L\rho_{L-1} \Big\|
            \A{L-1}F_\vecbf{\A{1, L-2}}(x_i) - \bA{L-1}F_\vecbf{\bA{1, L-2}}(x_i) 
        \Big\|_2 \\
\end{align*}

\noindent For $1\le k \le L-1$, construct the cover $\mathcal{C}_k$ with respect to the $\|.\|_2^S$ for the following class:
\begin{align*}
    \F_k = \bigCurl{
        x\mapsto \A{k}F_\vecbf{\A{1, k-1}}(x) : \|\A{k}\|_\sigma \le s_k, \ \|(\A{k} - \M{k})^T\|_{2, 1}\le a_i 
    }
\end{align*}

\noindent For every $x_i\in S$, we have:
\begin{align*}
    \Big\|\A{k}F_\vecbf{\A{1, k-1}}(x_i) &- \bA{k}F_\vecbf{\bA{1, k-1}}(x_i)\Big\|_2 \\
        &= \Big\|
            \bigRound{\A{k} - \bA{k}}F_\vecbf{\bA{1, k-1}}(x_i)
        \Big\|_2 + \Big\|
            \A{k}\bigRound{F_\vecbf{\A{1, k-1}}(x_i) - F_\vecbf{\bA{1, k-1}}(x_i)}
        \Big\|_2 \\
        &= \Big\|
            \bigRound{\A{k} - \bA{k}}F_\vecbf{\bA{1, k-1}}(x_i)
        \Big\|_2 + s_k\rho_{k-1}\Big\|
            \A{k-1}\bigRound{F_\vecbf{\A{1, k-2}}(x_i) - F_\vecbf{\bA{1, k-2}}(x_i)}
        \Big\|_2 \\
\end{align*}

\noindent Expand the above inductively in a similar manner as theorem \ref{thm:nn_cover_bound_with_general_norm} and for each layer index $1\le j\le k$, denote $\mathcal{B}_l=\bigCurl{\A{l}\in\R^{d_{l-1}\times d_l} : \|\A{l}\|_\sigma\le s_l, \|(\A{l}-\M{l})^T\|_{2, 1} \le a_l}$ and $\mathcal{C}_1$ is the cover of the first layer, we have:
\begin{align*}
    |\mathcal{C}_k| \le |\mathcal{C}_1| \cdot \prod_{j=2}^k \sup_{ 
        \substack{
            (\A{1}, \dots, \A{j-1}) \\
            \forall l \le j-1:\A{l}\in\mathcal{B}_l
        } 
    } \mathcal{N}\bigRound{
        \bigCurl{
            x\mapsto \A{j}F_\vecbf{\A{1, j-1}}(x) : \A{j}\in\mathcal{B}_j
        }, \epsilon_j, \|.\|_2^S 
    }
\end{align*}

\noindent Where $\epsilon_1, \dots, \epsilon_k$ are known positive constants. Denote $F_\vecbf{\A{1, j-1}}(x)=x$ for $j=1$ and $W=\max_{0\le l \le L-1}d_l$. By lemma 3.2 from \cite{article:bartlett}, we have:
\begin{align*}
    \log|\mathcal{C}_k| 
    &\le \sum_{j=1}^k\sup_{
        \substack{
            (\A{1}, \dots, \A{j-1}) \\ \forall l \le j-1: \A{l}\in\mathcal{B}_l
        }
    }\log\mathcal{N}\bigRound{
        \bigCurl{
            x \mapsto \A{j}F_\vecbf{\A{1, j-1}}(x) : \A{j} \in \mathcal{B}_j
        }, \epsilon_j, \|.\|_2^S
    } \\
    &\le \sum_{j=1}^k\sup_{
        \substack{
            (\A{1}, \dots, \A{j-1}) \\ \forall l \le j-1: \A{l}\in\mathcal{B}_l, \|x\|_2 \le 1
        }
    }\frac{a_j^2\|F_\vecbf{\A{1, j-1}}(x)\|_2^2}{\epsilon_j^2}\log(2d_{j-1}d_j) \\
    &\le \sum_{j=1}^k\sup_{
        \substack{
            (\A{1}, \dots, \A{j-1}) \\ \forall l \le j-1: \A{l}\in\mathcal{B}_l, \|x\|_2 \le 1
        }
    }\frac{a_j^2\|F_\vecbf{\A{1, j-1}}(x)\|_2^2}{\epsilon_j^2}\log(2W^2) \\
\end{align*}
