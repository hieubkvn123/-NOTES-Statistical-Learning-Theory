\newpage\section{Empirical Risk Minimization}

\subsection{Uniform Deviation Bounds}
\begin{definition}[Empirical Risk Minimization ($\widehat{h_n}$)]
    Let $\bigCurl{(X_i, Y_i)}_{i=1}^n$ be independently identically distributed random variables sampled from $P_{XY}$. Let $\mathcal{H}\subset \{0,1\}^{\mathcal{X}}$ be a set of classifiers. \textbf{Empirical Risk Minimization} is a learning algorithm such that:
    \begin{align*}
        \widehat{h_n} = \arg\min_{h\in\mathcal{H}} \widehat{R_n}(h)
    \end{align*}

    \noindent Where $\widehat{R_n}$ is the empirical risk and $\widehat{h_n}$ is called the \textbf{Empirical Risk Minimizer}. An important question is how close $\widehat{R_n}$ is to $R^*_\mathcal{H}=\inf_{h\in\mathcal{H}} R(h)$.
\end{definition}

\textbf{Overview (Uniform Deviation Bounds)} : Previously, we proved the following bound using the Hoeffding's inequality:
\begin{align*}
    P\bigRound{
        \bigAbs{
            \widehat{R_n}(h) - R(h)
        } \ge \epsilon
    } \le \delta
\end{align*}

\noindent Where $\delta = 2e^{-2n\epsilon^2}$. \textbf{However, since we do not know $\widehat{h_n}$ (the specific function in $\mathcal{H}$ that minimizes the empirical risk), we look for a bound that is guaranteed to apply for all $h\in\mathcal{H}$. This is called the Uniform Deviation Bound}.

\begin{definition}[Uniform Deviation Bounds (UDB)]
    Given a set of classifiers $\mathcal{H} \subset \{0,1\}^{\mathcal{X}}$, $\epsilon > 0$, the \textbf{Uniform Deviation Bounds} is the probability that for at least one $h\in\mathcal{H}$, the empirical risk deviates away from the true risk by $\epsilon$ and has the following form:
    \begin{align*}
        P\biggRound{
            \sup_{h\in\mathcal{H}}\bigAbs{\widehat{R_n}(h) - R(h)} \le \epsilon
        } &\ge 1 - \delta \\
        \text{Or : }
        P\biggRound{
            \sup_{h\in\mathcal{H}}\bigAbs{\widehat{R_n}(h) - R(h)} \ge \epsilon
        } &\le \delta
    \end{align*}

    \noindent The above bounds have the following interpretations:
    \begin{itemize}
        \item The probability that the deviation from the true risk is at most $\epsilon$ for all functions in $\mathcal{H}$ is at least $1-\delta$.
        \item The probability that there exists at least a function in $\mathcal{H}$ whose deviation from the true risk is at least $\epsilon$ is at most $\delta$.
    \end{itemize}

    \noindent Basically, we want to \textbf{bound the probability that some function deviates too far from the true risk}.
\end{definition}

\begin{proposition}{Uniform Deviation Bounds for finite $\mathcal{H}$}{udb_for_finite_H}
    Assume that $|\mathcal{H}| < \infty$. We have:
    \begin{align*}
        P\biggRound{
            \sup_{h\in\mathcal{H}}\bigAbs{\widehat{R_n}(h) - R(h)} \ge \epsilon
        } &\le 2|\mathcal{H}|e^{-2n\epsilon^2}
    \end{align*}
\end{proposition}

\begin{proof*}[Proposition \ref{prop:udb_for_finite_H}]
    For $h\in\mathcal{H}$, define the following event:
    \begin{align*}
        \Omega_\epsilon(h) = \bigCurl{
            \bigAbs{
                \widehat{R_n}(h) - R(h)
            } \ge \epsilon
        }
    \end{align*}

    \noindent Which is the event that the function $h$ deviates away from the true risk by $\epsilon>0$. Now, define the following event:
    \begin{align*}
        \Omega_\epsilon(\mathcal{H}) = \bigcup_{h\in\mathcal{H}} \Omega_\epsilon (h)
    \end{align*}

    \noindent Which is the event that at least one $h\in\mathcal{H}$ deviates away from the true risk by $\epsilon>0$. We have:
    \begin{align*}
        P\biggRound{
            \sup_{h\in\mathcal{H}}\bigAbs{\widehat{R_n}(h) - R(h)} \ge \epsilon
        } &= P(\Omega_\epsilon(\mathcal{H})) \\
        &= P\biggRound{
            \bigcup_{h\in\mathcal{H}} \Omega_\epsilon (h)
        } \\
        &\le \sum_{h\in\mathcal{H}} P(\Omega_\epsilon(h)) \\
        &\le \sum_{h\in\mathcal{H}} 2e^{-2n\epsilon^2} = 2|\mathcal{H}|e^{-2n\epsilon^2}
    \end{align*}
\end{proof*}

\begin{proposition}{(Probabilistic) Bound on Excess Risk of $\widehat{h_n}$}{deviation_from_rhstar}
    Suppose that $\mathcal{H}$ satisfies:
    \begin{align*}
        P\biggRound{
            \sup_{h\in\mathcal{H}}\bigAbs{\widehat{R_n}(h) - R(h)} \ge \epsilon
        } \le \delta
    \end{align*}

    \noindent Then, with probability of at least $1-\delta$, we have the following \textbf{upper bound on the Excess Risk of the Empirical Risk Minimizer}:
    \begin{align*}
        R(\widehat{h_n}) - R_\mathcal{H}^* \le 2\epsilon
    \end{align*}

    \noindent In other words, \textbf{with probability $1-\delta$, the \underline{empirical risk minimizer} deviates from the \underline{true risk minimizer} by at most $2\epsilon$}.
\end{proposition}

\begin{proof*}[Proposition \ref{prop:deviation_from_rhstar}]
    We have:
    \begin{align*}
        P\biggRound{
            \sup_{h\in\mathcal{H}}\bigAbs{\widehat{R_n}(h) - R(h)} \ge \epsilon
        } \le \delta \implies 
        P\biggRound{
            \sup_{h\in\mathcal{H}}\bigAbs{\widehat{R_n}(h) - R(h)} \le \epsilon
        } \ge 1 - \delta
    \end{align*}

    \noindent Hence, with probability $1-\delta$, for all $h\in\mathcal{H}$, we have:
    \begin{align*}
        \bigAbs{\widehat{R_n}(h) - R(h)} \le \epsilon &\implies -\epsilon \le \widehat{R_n}(h) - R(h) \le \epsilon \\
        &\implies \begin{cases}
            \widehat{R_n}(h) &\le R(h) + \epsilon 
            \\ \\
            R(h) &\le \widehat{R_n}(h) + \epsilon
        \end{cases}
    \end{align*}

    Therefore:
    \begin{align*}
        R(\widehat{h_n}) &\le \widehat{R_n}(\widehat{h_n}) + \epsilon \\
            &\le \widehat{R_n}(h) + \epsilon \ \ \ \text{(Since $\widehat{h_n}$ minimizes the Empirical Risk)} \\
            &\le \bigRound{R(h) + \epsilon} + \epsilon = R(h) + 2\epsilon
    \end{align*}

    \noindent Since $h\in\mathcal{H}$ is an arbitrary choice, we take the infimum over $\mathcal{H}$ to get the tightest bound. We have:
    \begin{align*}
        R(\widehat{h_n}) &\le \inf_{h\in\mathcal{H}} R(h) + 2\epsilon \\
            &= R_\mathcal{H}^* + 2\epsilon
    \end{align*}
\end{proof*}

\noindent \textbf{Remark} : We can express the above proposition verbally as "\textbf{If the UDB is at most $\delta$, then with probability $1-\delta$, the Excess Risk of the Empirical Risk Minimizer is at most $2\epsilon$}". 

\noindent \textbf{Remark} : Note that the above proof assumes that \textit{there exists an empirical risk minimizer}. This is not guaranteed when $|\mathcal{H}|$ is infinite.

\begin{proposition}{(Non-probabilistic) Bound on Excess Risk of $\widehat{h_n}$}{nprob_deviation_from_rhstar}
    We have the following inequality:
    \begin{align*}
        R(\widehat{h_n}) - R_\mathcal{H}^* \le 2\sup_{h\in\mathcal{H}} \bigAbs{
            \widehat{R_n}(h) - R(h)
        }
    \end{align*}
\end{proposition}

\begin{proof*}[Proposition \ref{prop:nprob_deviation_from_rhstar}]
    Let $h_\mathcal{H}^* = \arg\min_{h\in\mathcal{H}}R(h)$. We have:
    \begin{align*}
        R(\widehat{h_n}) - R_\mathcal{H}^* 
            &\le 
                \bigAbs{R(\widehat{h_n}) - \widehat{R_n}(\widehat{h_n})} 
                + \widehat{R_n}(\widehat{h_n}) - \widehat{R_n}(h_\mathcal{H}^*) 
                + \bigAbs{\widehat{R_n}(h_\mathcal{H}^*) - R_\mathcal{H}^*}
    \end{align*}

    \noindent Since $\widehat{h_n}$ is the Empirical Risk Minimizer, we have $\widehat{R_n}(\widehat{h_n}) - \widehat{R_n}(h_\mathcal{H}^*) \le 0$. Hence:
    \begin{align*}
        R(\widehat{h_n}) - R_\mathcal{H}^* 
            &\le   
                \bigAbs{R(\widehat{h_n}) - \widehat{R_n}(\widehat{h_n})} 
                + \bigAbs{\widehat{R_n}(h_\mathcal{H}^*) - R_\mathcal{H}^*} \\
            &\le 
                2\sup_{h\in\mathcal{H}}\bigAbs{
                    \widehat{R_n}(h) - R(h)
                }
    \end{align*}
\end{proof*}

\begin{corollary}{Excess Risk of $\widehat{h_n}$ - $\delta \to \epsilon$ relation}{excess_risk_erm_epsilon_delta}
    This is a Corollary for both proposition \ref{prop:deviation_from_rhstar} and proposition \ref{prop:nprob_deviation_from_rhstar}. If $\mathcal{H}$ is finite, then:
    \begin{align*}
        P\bigRound{
            R(\widehat{h_n}) - R_\mathcal{H}^* \ge \epsilon
        } \le \underbrace{2|\mathcal{H}|e^{-n\epsilon^2/2}}_{\delta}
    \end{align*}

    \noindent Equivalently, with probability of at least $1-\delta$, we have:
    \begin{align*}
        R(\widehat{h_n}) \le R_\mathcal{H}^* + \sqrt{
            \frac{2}{n}\biggRound{\log|\mathcal{H}| - \log\frac{\delta}{2}}
        }
    \end{align*}
\end{corollary}

\begin{proof*}[Corollary \ref{coro:excess_risk_erm_epsilon_delta}]
    By proposition \ref{prop:nprob_deviation_from_rhstar}, we have:
    \begin{align*}
        P\bigRound{
            R(\widehat{h_n}) - R_\mathcal{H}^* \ge \epsilon
        } 
        &\le
        P\biggRound{2\sup_{h\in\mathcal{H}} \bigAbs{\widehat{R_n}(h) - R(h)} \ge \epsilon) } \\
        &= P\biggRound{\sup_{h\in\mathcal{H}} \bigAbs{\widehat{R_n}(h) - R(h)} \ge \frac{\epsilon}{2})} \\
        &\le 2|\mathcal{H}|\exp\biggRound{
            -\frac{n\epsilon^2}{2}
        }
    \end{align*}

    \noindent Now, let:
    \begin{align*}
        \delta = 2|\mathcal{H}|\exp\biggRound{
            -\frac{n\epsilon^2}{2}
        } \implies \epsilon = \sqrt{
            \frac{2}{n}\biggRound{\log|\mathcal{H}| - \log\frac{\delta}{2}}
        }
    \end{align*}

    \noindent By proposition \ref{prop:deviation_from_rhstar}, with at least probability $1-\delta$, we have:
    \begin{align*}
        R(\widehat{h_n}) \le R_\mathcal{H}^* + \epsilon = R_\mathcal{H}^* + \sqrt{
            \frac{2}{n}\biggRound{\log|\mathcal{H}| - \log\frac{\delta}{2}}
        }
    \end{align*}
\end{proof*}


\subsection{PAC Learning \& Sample Complexity}
\begin{definition}[Sample Complexity ($N(\epsilon, \delta)$)]
    We say that $\widehat{h_n}$ is a \textbf{$(\epsilon, \delta)$-learning algorithm} for $\mathcal{H}$ if there exists a function $N(\epsilon, \delta)$ such that: 
    \begin{align*}
        \forall \epsilon, \delta > 0 : n \ge N(\epsilon, \delta) \implies 
        P\bigRound{R(\widehat{h_n}) - R_\mathcal{H}^* \ge \epsilon} \le \delta
    \end{align*}   

    \noindent Where we have:
    \begin{itemize}
        \item $N(\epsilon, \delta)$ is called the \textbf{Sample Complexity}.
        \item $\mathcal{H}$ is called \textbf{Uniformly Learnable}.
        \item $\widehat{h_n}$ is called \textbf{Probably Approximately Correct (PAC)}.
    \end{itemize}
\end{definition}
\textbf{Remark} : By corollary \ref{coro:excess_risk_erm_epsilon_delta}, we have $\delta = 2|\mathcal{H}|\exp\biggRound{-\frac{n\epsilon^2}{2}}$. Solving for $n$, we have:
\begin{align*}
    N(\epsilon, \delta) = \frac{2}{\epsilon^2}\biggRound{
        \log|\mathcal{H}| - \log\frac{\delta}{2}
    }
\end{align*}



\subsection{Zero-error case}

