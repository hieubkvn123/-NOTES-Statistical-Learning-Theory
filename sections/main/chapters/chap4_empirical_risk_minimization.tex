\newpage\section{Empirical Risk Minimization}

\subsection{Uniform Deviation Bounds}
\begin{definition}[Empirical Risk Minimization]
    Let $\bigCurl{(X_i, Y_i)}_{i=1}^n$ be independently identically distributed random variables sampled from $P_{XY}$. Let $\mathcal{H}\subset \{0,1\}^{\mathcal{X}}$ be a set of classifiers. \textbf{Empirical Risk Minimization} is a learning algorithm such that:
    \begin{align*}
        \widehat{h_n} = \arg\min_{h\in\mathcal{H}} \widehat{R_n}(h)
    \end{align*}

    \noindent Where $\widehat{R_n}$ is the empirical risk. An important question is how close $\widehat{R_n}$ is to $R^*_\mathcal{H}=\inf_{h\in\mathcal{H}} R(h)$.
\end{definition}

\textbf{Overview (Uniform Deviation Bounds)} : Previously, we proved the following bound using the Hoeffding's inequality:
\begin{align*}
    P\bigRound{
        \bigAbs{
            \widehat{R_n}(h) - R(h)
        } \ge \epsilon
    } \le \delta
\end{align*}

\noindent Where $\delta = 2e^{-2n\epsilon^2}$. \textbf{However, since we do not know $\widehat{h_n}$ (the specific function in $\mathcal{H}$ that minimizes the empirical risk), we look for a bound that is guaranteed to apply for all $h\in\mathcal{H}$. This is called the Uniform Deviation Bound}.

\begin{definition}[Uniform Deviation Bounds (UDB)]
    Given a set of classifiers $\mathcal{H} \subset \{0,1\}^{\mathcal{X}}$, $\epsilon > 0$, the \textbf{Uniform Deviation Bounds} has the following form:
    \begin{align*}
        P\biggRound{
            \sup_{h\in\mathcal{H}}\bigAbs{\widehat{R_n}(h) - R(h)} \le \epsilon
        } &\ge 1 - \delta \\
        \text{Or : }
        P\biggRound{
            \sup_{h\in\mathcal{H}}\bigAbs{\widehat{R_n}(h) - R(h)} \ge \epsilon
        } &\le \delta
    \end{align*}

    \noindent The above bounds have the following interpretations:
    \begin{itemize}
        \item The probability that the deviation from the true risk is at most $\epsilon$ for all functions in $\mathcal{H}$ is at least $1-\delta$.
        \item The probability that there exists at least a function in $\mathcal{H}$ whose deviation from the true risk is at least $\epsilon$ is at most $\delta$.
    \end{itemize}

    \noindent Basically, we want to \textbf{bound the probability that some function deviates too far from the true risk}.
\end{definition}

\begin{proposition}{Uniform Deviation Bounds for finite $\mathcal{H}$}{udb_for_finite_H}
    Assume that $|\mathcal{H}| < \infty$. We have:
    \begin{align*}
        P\biggRound{
            \sup_{h\in\mathcal{H}}\bigAbs{\widehat{R_n}(h) - R(h)} \ge \epsilon
        } &\le 2|\mathcal{H}|e^{-2n\epsilon^2}
    \end{align*}
\end{proposition}

\begin{proof*}[Proposition \ref{prop:udb_for_finite_H}]
    For $h\in\mathcal{H}$, define the following event:
    \begin{align*}
        \Omega_\epsilon(h) = \bigCurl{
            \bigAbs{
                \widehat{R_n}(h) - R(h)
            } \ge \epsilon
        }
    \end{align*}

    \noindent Which is the event that the function $h$ deviates away from the true risk by $\epsilon>0$. Now, define the following event:
    \begin{align*}
        \Omega_\epsilon(\mathcal{H}) = \bigcup_{h\in\mathcal{H}} \Omega_\epsilon (h)
    \end{align*}

    \noindent Which is the event that at least one $h\in\mathcal{H}$ deviates away from the true risk by $\epsilon>0$. We have:
    \begin{align*}
        P\biggRound{
            \sup_{h\in\mathcal{H}}\bigAbs{\widehat{R_n}(h) - R(h)} \ge \epsilon
        } &= P(\Omega_\epsilon(\mathcal{H})) \\
        &= P\biggRound{
            \bigcup_{h\in\mathcal{H}} \Omega_\epsilon (h)
        } \\
        &\le \sum_{h\in\mathcal{H}} P(\Omega_\epsilon(h)) \\
        &\le \sum_{h\in\mathcal{H}} 2e^{-2n\epsilon^2} = 2|\mathcal{H}|e^{-2n\epsilon^2}
    \end{align*}
\end{proof*}

\begin{proposition}{Deviation from $R_\mathcal{H}^*$}{deviation_from_rhstar}
    Suppose that $\mathcal{H}$ satisfies:
    \begin{align*}
        P\biggRound{
            \sup_{h\in\mathcal{H}}\bigAbs{\widehat{R_n}(h) - R(h)} \ge \epsilon
        } \le \delta
    \end{align*}

    \noindent Then, with probability of at least $1-\delta$, we have:
    \begin{align*}
        R(\widehat{h_n}) - R_\mathcal{H}^* \le 2\epsilon
    \end{align*}

    \noindent In other words, \textbf{with probability $1-\delta$, the \underline{empirical risk minimizer} deviates from the \underline{true risk minimizer} by at most $2\epsilon$}.
\end{proposition}

\begin{proof*}[Proposition \ref{prop:deviation_from_rhstar}]
    We have:
    \begin{align*}
        P\biggRound{
            \sup_{h\in\mathcal{H}}\bigAbs{\widehat{R_n}(h) - R(h)} \ge \epsilon
        } \le \delta \implies 
        P\biggRound{
            \sup_{h\in\mathcal{H}}\bigAbs{\widehat{R_n}(h) - R(h)} \le \epsilon
        } \ge 1 - \delta
    \end{align*}

    \noindent Hence, with probability $1-\delta$, for all $h\in\mathcal{H}$, we have:
    \begin{align*}
        \bigAbs{\widehat{R_n}(h) - R(h)} \le \epsilon &\implies -\epsilon \le \widehat{R_n}(h) - R(h) \le \epsilon \\
        &\implies \begin{cases}
            \widehat{R_n}(h) &\le R(h) + \epsilon 
            \\ \\
            R(h) &\le \widehat{R_n}(h) + \epsilon
        \end{cases}
    \end{align*}

    Therefore:
    \begin{align*}
        R(\widehat{h_n}) &\le \widehat{R_n}(\widehat{h_n}) + \epsilon \\
            &\le \widehat{R_n}(h) + \epsilon \ \ \ \text{(Since $\widehat{h_n}$ minimizes the Empirical Risk)} \\
            &\le \bigRound{R(h) + \epsilon} + \epsilon = R(h) + 2\epsilon
    \end{align*}

    \noindent Since $h\in\mathcal{H}$ is an arbitrary choice, we take the infimum over $\mathcal{H}$ to get the tightest bound. We have:
    \begin{align*}
        R(\widehat{h_n}) &\le \inf_{h\in\mathcal{H}} R(h) + 2\epsilon \\
            &= R_\mathcal{H}^* + 2\epsilon
    \end{align*}
\end{proof*}

\subsection{PAC Learning \& Sample Complexity}

\subsection{Zero-error case}

