\newpage\section{Rademacher Complexity}
\subsection{Bounded Difference Inequality}
In the following section, we will discuss another concentration inequality that bounds the difference between functions of random sample and their mean given that the functions satisfy the \textbf{bounded difference property}.

\begin{definition}[Bounded difference property]
    Given a real-valued function $\phi:\X^n\to\R$. We say that $\phi$ satisfies the \textbf{bounded difference property} if $\exists c_1, \dots, c_n\in\X$ such that $\forall 1 \le i \le n$:
    \begin{align*}
        \sup_{\{x_1, \dots, x_n\}\subset\X, x_i' \in \X}
        \biggAbs{
            \phi(x_1, \dots, x_i, \dots, x_n) - \phi(x_1, \dots, x_i', \dots, x_n)
        } \le c_i
    \end{align*}

    \noindent That is, substituting the value at the $i^{th}$ coordinate $x_i$ changes the value of $\phi$ by at most $c_i$.
\end{definition}

\begin{theorem}{Bounded Difference (McDiarmid's) Inequality}{bounded_diff_inequality}
    Let $X_1, \dots, X_n$ be independent random variables (not necessarily identically distributed) and $\phi:\X^n \to \R$ be a function satisfying the bounded difference property:
    \begin{align*}
        \sup_{\{x_1, \dots, x_n\}\subset\X, x_i' \in \X}
        \biggAbs{
            \phi(x_1, \dots, x_i, \dots, x_n) - \phi(x_1, \dots, x_i', \dots, x_n)
        } \le c_i, \ \forall 1\le i \le n
    \end{align*}

    \noindent Then, we have:
    \begin{align*}
        P\biggRound{
            \bigAbs{
                \phi(X_1, \dots, X_n) - \mathbb{E}\bigSquare{ \phi(X_1, \dots, X_n) }
            } \ge t
        } \le 2\exp\biggRound{
            -\frac{2t^2}{\sum_{i=1}^n c_i^2}
        }, \ \forall t > 0
    \end{align*}
\end{theorem}

\noindent\textbf{Remark} : Assume that $X_i\in[a_i, b_i]$ and $\phi(X_1, \dots, X_n)=\sum_{i=1}^n X_i$. Then the bounded difference inequality recovers the Hoeffding's inequality \ref{thm:hoeffding_inequality}.

\begin{proof*}[Theorem \ref{thm:bounded_diff_inequality}]
    Define the following random variable:
    \begin{align*}
        V_i = \mathbb{E}\Big[\phi\Big|X_1, \dots, X_i\Big] - \mathbb{E}\Big[\phi\Big|X_1, \dots, X_{i-1}\Big]
    \end{align*}

    \noindent Denote $\phi(X_1, \dots, X_n)=\phi$ and $\mathbb{E}[\phi(X_1, \dots, X_n)] = \mu_\phi$ for brevity, we have:
    \begin{align*}
        \phi - \mu_\phi = \sum_{i=1}^n V_i
    \end{align*}

    \noindent Using the Chernoff's bounding method, we have:
    \begin{align*}
        P(\phi - \mu_\phi \ge t) 
            &\le \inf_{s>0}e^{-st}M_{\phi-\mu_\pi}(s) \\
            &= \inf_{s>0}e^{-st}\mathbb{E}\biggSquare{
                \exp\biggRound{
                    s\sum_{i=1}^n V_i
                }
            }
    \end{align*}

    \begin{subproof}{\newline Claim 1 : For all $1\le i\le n$, $a_i\le V_i \le b_i$ and $b_i-a_i\le c_i$.}
        Define the infimum and supremum of $V_i$ as followed:

        \begin{align*}
            U_i &= \sup_{x\in\X}\biggCurl{
                \E\bigSquare{\phi \Big| X_1, \dots, X_i=x} - \E\bigSquare{\phi\Big|X_1, \dots, X_{i-1}}
            } \\
            L_i &= \inf_{x\in\X}\biggCurl{
                \E\bigSquare{\phi \Big| X_1, \dots, X_i=x} - \E\bigSquare{\phi\Big|X_1, \dots, X_{i-1}}
            }
        \end{align*}

        \noindent Clearly, $U_i \ge V_i \ge L_i$. We have:
        \begin{align*}
            U_i - L_i
                &= \sup_{x\in\X}\E\bigSquare{\phi\Big|X_1, \dots, X_{i-1}, X_i=x} - \inf_{x\in\X}\E\bigSquare{\phi\Big|X_1, \dots, X_{i-1}, X_i=x} \\
                &= \sup_{x\in\X}\int\phi(X_1, \dots, X_{i-1}, x, x_{i+1}, \dots, x_n)dP(x_{i+1}, \dots, x_n|X_1, \dots, X_{i-1}, x) \\
                & \ \ \ -\inf_{x\in\X}\int\phi(X_1, \dots, X_{i-1}, x, x_{i+1}, \dots, x_n)dP(x_{i+1}, \dots, x_n|X_1, \dots, X_{i-1}, x) \\
                &= \sup_{x\in\X}\int\phi(X_1, \dots, X_{i-1}, x, x_{i+1}, \dots, x_n)dP(x_{i+1}, \dots, x_n) \\
                & \ \ \ -\inf_{x\in\X}\int\phi(X_1, \dots, X_{i-1}, x, x_{i+1}, \dots, x_n)dP(x_{i+1}, \dots, x_n) \\
                &= \sup_{x, y\in \X} \int\bigSquare{
                    \phi(X_1, \dots, X_{i-1}, x, x_{i+1}, \dots, x_n) - \phi(X_1, \dots, X_{i-1}, y, x_{i+1}, \dots, x_n)
                }dP(x_{i+1}, \dots, x_n) \\
                &\le c_i \int dP(x_{i+1}, \dots, x_n) = c_i
        \end{align*}
    \end{subproof}

    \begin{subproof}{\newline Claim 2 : $\E\bigSquare{ V_i\Big| X_1, \dots, X_{i-1} } = 0, \ \forall 1 \le i \le n$.}
        We have:
        \begin{align*}
            \E\bigSquare{ V_i\Big| X_1, \dots, X_{i-1} } 
                &= \E\biggSquare{ \E\bigSquare{\phi\Big|X_1, \dots, X_i} \Bigg| X_1, \dots, X_{i-1} }
                - \E\bigSquare{ \E\bigSquare{\phi\Big|X_1, \dots, X_{i-1}} \Big| X_1, \dots, X_{i-1} } \\
                &= \E\biggSquare{ \E\bigSquare{\phi\Big|X_1, \dots, X_i} \Bigg| X_1, \dots, X_{i-1} }
                - \E\bigSquare{\phi\Big|X_1, \dots, X_{i-1}}
        \end{align*}

        \noindent By the tower property, we have:
        \begin{align*}
            \E\biggSquare{ \E\bigSquare{\phi\Big|X_1, \dots, X_i} \Bigg| X_1, \dots, X_{i-1} } = \E\bigSquare{\phi\Big|X_1, \dots, X_{i-1}}
        \end{align*}

        \noindent Hence, 
        \begin{align*}
            \E\bigSquare{ V_i\Big| X_1, \dots, X_{i-1} } &= \E\bigSquare{\phi\Big|X_1, \dots, X_{i-1}} - \E\bigSquare{\phi\Big|X_1, \dots, X_{i-1}} \\ &=0
        \end{align*}
    \end{subproof}

    \noindent From the above claims, we can make use of the Hoeffding's lemma \ref{lem:hoeffding_lemma} to bound the moment generating functions. We have:
    \begin{align*}
        P(\phi - \mu_\phi \ge t) 
        &= \inf_{s>0}e^{-st}\mathbb{E}\biggSquare{
            \exp\biggRound{
                s\sum_{i=1}^n V_i
            }
        } \\
        &= \inf_{s>0}e^{-st}\E_{X_1, \dots, X_{n-1}}\E_{X_n|X_1, \dots, X_{n-1}}\biggSquare{
            \exp\biggRound{
                s\sum_{i=1}^n V_i
            }
        } \\
        &= \inf_{s>0}e^{-st}\E_{X_n|X_1, \dots, X_{n-1}}\bigSquare{e^{sV_n}}\E_{X_1, \dots, X_{n-1}}\biggSquare{
            \exp\biggRound{
                s\sum_{i=1}^{n-1} V_i
            }
        }\\
        &\le \inf_{s>0}\exp\biggRound{
            -st + \frac{s^2c_n^2}{8}
        }\E_{X_1, \dots, X_{n-1}}\biggSquare{
            \exp\biggRound{
                s\sum_{i=1}^{n-1} V_i
            }
        }  \ \ \ \text{(Lemma \ref{lem:hoeffding_lemma})} \\
        &\vdots \\
        &\le \inf_{s>0}\exp\biggRound{
            -st + s^2 \sum_{i=1}^n \frac{c_i^2}{8}
        }
    \end{align*}

    \noindent Substituting $s=\frac{4t}{\sum_{i=1}^n c_i^2}$ to minimize the upperbound (just like the proof for Hoeffding's inequality \ref{thm:hoeffding_inequality}). We have:
    \begin{align*}
        P(\phi - \mu_\phi \ge t) \le \exp\biggRound{
            -\frac{2t^2}{\sum_{i=1}^n c_i^2}
        }
    \end{align*}
\end{proof*}

\subsection{Rademacher Complexity}
\textbf{Overview} : Rademacher Complexity is a measure for the richness of a class of real-valued functions. In this sense, it is similar to VC dimension. However, unlike VC dimension, the Rademacher Complexity is not restricted to binary functions.

\begin{definition}[Empirical Rademacher Complexity]
    Let $\G\subseteq[a,b]^{\mathcal{Z}}$ be a set of functions $\mathcal{Z} \to [a, b]$ where $a,b\in\R, a < b$. Let $Z_1, \dots, Z_n$ be an independently identically distributed random sample on $\mathcal{Z}$ following some distribution $P$. Denote $S=(Z_1, \dots, Z_n)$, we define the \textbf{Empirical Rademacher Complexity} as:
    \begin{align*}
        \ERC_S(\G) = \E_\sigma\biggSquare{
            \sup_{g\in\G} \frac{1}{n}\sum_{i=1}^n \sigma_i g(Z_i)
        }
    \end{align*}

    \noindent Where $\sigma=(\sigma_1, \dots, \sigma_n)^T$, $\sigma_i\sim Uniform(-1, 1)$ are known as \textbf{Rademacher random variables}. Note that $\ERC_S(\G)$ is random due to randomness in $S$.
\end{definition}

\begin{definition}[Rademacher Complexity]
    The \textbf{Rademacher Complexity} of a function class $\G$ is defined as:
    \begin{align*}
        \RC_n(\G) = \E_S\bigSquare{ \ERC_S(\G) }
    \end{align*}
\end{definition}

\begin{theorem}{One-sided Rademacher Complexity bound}{one_sided_rademacher_bound}
    Let $Z$ be a random variable and $S=(Z_1, \dots, Z_n)$ be an independently identically distributed sample over $\mathcal{Z}$. Consider a class of functions $\G\subseteq [a,b]^\mathcal{Z}$. $\forall \delta > 0, g\in\G$, with at least probability $1-\delta$ with respect to the draw of sample $S$, we have:
    \begin{align*}
        {\bf (i)} \ \ \sup_{g\in\G}\biggCurl{\E\bigSquare{g(Z)} - \frac{1}{n} \sum_{i=1}^n g(Z_i)} &\le 2\RC_n(\G) + (b-a)\sqrt{\frac{\log1/\delta}{2n}} \\
        {\bf (ii)} \  \sup_{g\in\G}\biggCurl{\E\bigSquare{g(Z)} - \frac{1}{n} \sum_{i=1}^n g(Z_i)} &\le 2\ERC_S(\G) + 3(b-a)\sqrt{\frac{\log2/\delta}{2n}}
    \end{align*}
\end{theorem}

\begin{proof*}[Theorem \ref{thm:one_sided_rademacher_bound}]
    For notation brevity, define:
    \begin{align*}
        \widehat{\E_S}[g] &= \frac{1}{n}\sum_{i=1}^n g(Z_i); \ \E\bigSquare{g(Z)} = \E[g] \\
    \end{align*}
    
    \begin{subproof}{\newline ${\bf (i)} \ \ \E[g] - \widehat{\E_S}[g] \le 2\RC_n(\G) + (b-a)\sqrt{\frac{\log1/\delta}{2n}}$}
        Define the following function:
        \begin{align*}
            \phi(S) = \sup_{g\in\G} \bigCurl{
                \E[g] - \widehat{\E_S}[g]
            }
        \end{align*}

        \noindent First, we check that $\phi$ has the bounded difference property. Define $S'_i=(Z_1, \dots, Z_i', \dots, Z_n)$, we have:
        \begin{align*}
            \bigAbs{\phi(S) - \phi(S'_i)} 
                &= \biggAbs{
                    \sup_{g\in\G} \bigCurl{\E[g] - \widehat{E_S}[g]} - \sup_{g\in\G}\bigCurl{\E[g] - \widehat{\E_{S_i'}}[g]}
                } \\
                &\le \biggAbs{
                    \sup_{g\in\G} \bigCurl{ \widehat{\E_{S_i'}}[g] - \widehat{E_S}[g] }
                } \ \ \ \Big(\sup A - \sup B \le \sup \bigCurl{A - B} \Big) \\
                &= \biggAbs{
                    \frac{1}{n} \sup_{g\in\G} \bigRound{ g(Z_i') - g(Z_i) }
                } \\
                &\le \frac{b-a}{n}
        \end{align*}

        \noindent By the Bounded Difference Inequality, we have:
        \begin{align*}
            P\biggRound{
                \phi(S) - \E[\phi(S)] \le t
            } &\ge 1 - \exp\biggRound{
                -\frac{2t^2}{\sum_{i=1}^n (b - a)^2/n^2}
            }, \ \ \ t\ge 0 \\
            &= 1 - \exp\biggRound{
                -\frac{2nt^2}{(b - a)^2}
            }
        \end{align*}

        \noindent Now, let:
        \begin{align*}
            \delta = \exp\biggRound{
                -\frac{2nt^2}{(b - a)^2}
            } \implies t = (b-a)\sqrt{
                \frac{\log 1/\delta}{2n}
            }
        \end{align*}

        \noindent Hence, for all $\delta>0$, with probability of at least $1-\delta$, we have:
        \begin{align*}
            \phi(S) \le \E[\phi(S)] + (b-a)\sqrt{
                \frac{\log 1/\delta}{2n}
            } \ \ \ (1)
        \end{align*}
    \end{subproof}

    \noindent Now, to establish ${\bf (i)}$, we have to show that $\E[\phi(S)] \le 2\RC_n(\G)$. Let $S'=(Z_1', \dots, Z_n')$. We have:
    \begin{align*}
        \E[\phi(S)] 
            &= \E_S\biggSquare{ \sup_{g\in\G} \bigCurl{ \E[g] - \widehat{E_S}[g] } } \\
            &= \E_S\biggSquare{
                \sup_{g\in\G} \bigCurl{
                    E_{S'} \bigSquare{ \widehat{E_{S'}}[g] - \widehat{E_S}[g]}
                }
            } \ \ \ \Big(\E[g] = \E_{S'}\widehat{E_{S'}}[g]\Big) \\
            &\le \E_{S, S'} \biggSquare{
                \sup_{g\in\G} \bigCurl{
                    \widehat{E_{S'}}[g] - \widehat{E_S}[g]
                }
            } \ \ \ \Big( \text{Jensen's Inequality : } \sup \E \le \E\sup \Big) \\
            &= \E_{S, S'} \biggSquare{
                \sup_{g\in\G} \frac{1}{n}\sum_{i=1}^n \bigRound{
                    g(Z_i') - g(Z_i)
                }
            } \\
            &= \E_{\sigma, S, S'} \biggSquare{
                \sup_{g\in\G} \frac{1}{n}\sum_{i=1}^n \sigma_i\bigRound{
                    g(Z_i') - g(Z_i)
                }
            } \ \ \ (Z_i, Z_i' \text{ are i.i.d, }\sigma_i \text{ are symmetric}) \\
            &\le E_{\sigma, S'} \biggSquare{
                \sup_{g\in\G} \frac{1}{n}\sum_{i=1}^n \sigma_ig(Z_i')
            } + E_{\sigma, S} \biggSquare{
                \sup_{g\in\G} \frac{1}{n}\sum_{i=1}^n (-\sigma_i)g(Z_i)
            } \ \ \ (\sup(f_1 + f_2) \le \sup f_1 + \sup f_2) \\
            &=  E_{\sigma, S'} \biggSquare{
                \sup_{g\in\G} \frac{1}{n}\sum_{i=1}^n \sigma_ig(Z_i')
            } + E_{\sigma, S} \biggSquare{
                \sup_{g\in\G} \frac{1}{n}\sum_{i=1}^n \sigma_ig(Z_i)
            } \ \ \ (\text{Rademacher variables are symmetric})  \\
            &= 2\RC_n(\G) \ \ \ (2)
    \end{align*}

    \noindent From $(1)$ and $(2)$, we have:
    \begin{align*}
        \phi(S) \le 2\RC_n(\G) + (b-a)\sqrt{
            \frac{\log 1/\delta}{2n}
        }
    \end{align*}

    \begin{subproof}{\newline ${\bf (ii)} \ \ \E[g] - \widehat{\E_S}[g] \le 2\ERC_n(\G) + 3(b-a)\sqrt{\frac{\log2/\delta}{2n}}$}
        We will first verify that the Empirical Rademacher Complexity satisfies the bounded difference property. Let $S=(X_1, \dots, X_n)$ and $S_i'=(Y_1, \dots, Y_n)$ such that $Y_j=X_j, \forall j \ne i, 1 \le i \le n$. We have:
        \begin{align*}
            \bigAbs{ \ERC_S(\G) - \ERC_{S_i'}(\G) } 
                &\le \biggAbs{
                    \E_\sigma\biggSquare{
                        \sup_{g\in\G} \frac{1}{n} \biggRound{
                            \sum_{j=1}^n \sigma_j g(X_j) - \sum_{j=1}^n \sigma_j g(Y_j)
                        }
                    }
                } \\
                &= \biggAbs{
                    \E_\sigma\biggSquare{
                        \sup_{g\in\G} \frac{1}{n} \sum_{j=1}^n \sigma_j (g(X_j) - g(Y_j))
                    }
                } \\
                &= \biggAbs{
                    \E_\sigma\biggSquare{
                        \sup_{g\in\G} \frac{1}{n} \sum_{j=1}^n (g(X_j) - g(Y_j))
                    }
                } \ \ \ (\sigma_i \text{ are symmetric}) \\
                &= \biggAbs{
                    \E_\sigma\biggSquare{
                        \sup_{g\in\G} \frac{1}{n} \bigRound{
                            g(X_i) - g(Y_i)
                        }
                    }
                } \\
                &\le \frac{b-a}{n}
        \end{align*}

        \noindent Therefore, by the bounded difference inequality, with at least $1-\delta/2$ probability, we have:
        \begin{align*}
            \ERC_S(\G) - \E_S\bigSquare{\ERC_S(\G)} &\ge (a-b)\sqrt{\frac{\log 2/\delta}{2n}} \\
            \implies \ERC_S(\G) - \RC_n(\G) &\ge (a-b)\sqrt{\frac{\log 2/\delta}{2n}} \\
            \implies \RC_n(\G)  &\le \ERC_S(\G) + (b-a)\sqrt{\frac{\log 2/\delta}{2n}}
        \end{align*}

        \noindent From ${\bf (i)}$ we also have, with a probability of at least $1-\delta/2$, we have:
        \begin{align*}
            \phi(S) \le 2\RC_n(\G) + (b-a)\sqrt{
                \frac{\log 2/\delta}{2n}
            }
        \end{align*}

        \noindent Now denote the following events:
        \begin{align*}
            A &:= \biggCurl{
                \RC_n(\G) \le \ERC_S(\G) + (b-a)\sqrt{ \frac{\log 2/\delta}{2n} }
            } \\
            B &:= \biggCurl{
                \phi(S) \le 2 \RC_n(\G) + (b-a)\sqrt{ \frac{\log 2/\delta}{2n} }
            }
        \end{align*}

        \noindent We have:
        \begin{align*}
            P\biggRound{\biggCurl{
                \phi(S) \le 2\ERC_S(\G) + 3(b-a)\sqrt{
                    \frac{\log 2/\delta}{2n}
                }
            }} &\ge P(A\cap B) \\
            &= 1 - P(\overline{A} \cup \overline{B}) \\
            &\ge 1 - \bigRound{P(\overline{A}) + P(\overline{B})} \\
            &= 1 - (\delta/2 + \delta/2) = 1 - \delta
        \end{align*}
    \end{subproof}
\end{proof*}

\begin{theorem}{Two-sided Rademacher Complexity bound}{two_sided_rademacher_bound}
    Consider a set of classifiers $\G\subseteq[a,b]^\mathcal{Z}$. Then, $\forall \delta>0$, with probability of at least $1-\delta$ with respect to the draw of sample $S$, we have:
    \begin{align*}
        \sup_{g\in\G}\biggAbs{
            \E\bigSquare{ g(Z) } - \frac{1}{n}\sum_{i=1}^n g(Z_i)
        } &\le 2 \RC_n(\G) + (b-a)\sqrt{
            \frac{\log 2/\delta}{2n}
        } \\
        \sup_{g\in\G}\biggAbs{
            \E\bigSquare{ g(Z) } - \frac{1}{n}\sum_{i=1}^n g(Z_i)
        } &\le 2 \ERC_n(\G) + 3(b-a)\sqrt{
            \frac{\log 4/\delta}{2n}
        }
    \end{align*}
\end{theorem}

\begin{proof*}[Theorem \ref{thm:two_sided_rademacher_bound}]
    Note that we have:
    \begin{align*}
        P\biggRound{
            \sup|A| < \epsilon
        } &= P\biggRound{
            \Big\{ \sup A < \epsilon \Big\} \cap \Big\{\sup -A < \epsilon \Big\}
        } \\
        \implies
        P\biggRound{
            \sup|A| < \epsilon
        } &= 1 - P\biggRound{
            \Big\{ \sup A \ge \epsilon \Big\} \cup \Big\{\sup -A \ge \epsilon \Big\}
        } \\
        &\ge 1 - \Bigg[ P\bigRound{\bigCurl{\sup A \ge \epsilon}} + P\bigRound{\bigCurl{\sup -A \ge \epsilon}} \Bigg]
    \end{align*}

    \noindent Repeating the same arguments as the one-sided Rademacher complexity bounds proof in \ref{thm:one_sided_rademacher_bound}, we can prove that with probability of at least $1-\delta$, we have:
    \begin{align*}
        \sup_{g\in\G} \biggCurl{
            \frac{1}{n}\sum_{i=1}^n g(Z_i) - \E\bigSquare{ g(Z) }
        } &\le 2\RC_n(\G) + (b-a)\sqrt{
            \frac{\log 1/\delta}{2n}
        } \\
        \sup_{g\in\G} \biggCurl{
            \frac{1}{n}\sum_{i=1}^n g(Z_i) - \E\bigSquare{ g(Z) }
        } &\le 2\ERC_S(\G) + 3(b-a)\sqrt{
            \frac{\log 2/\delta}{2n}
        }
    \end{align*}

    \noindent Changing the probability from $1-\delta$ to $1-\delta/2$ and combine the above bounds with the bounds in theorem \ref{thm:one_sided_rademacher_bound}, we have:
    \begin{align*}
        \sup_{g\in\G} \biggAbs{
            \E\bigSquare{ g(Z) } - \frac{1}{n}\sum_{i=1}^n g(Z_i)
        } &\le 2\RC_n(\G) + (b-a)\sqrt{
            \frac{\log 2/\delta}{2n}
        } \\
        \sup_{g\in\G} \biggAbs{
            \E\bigSquare{ g(Z) } - \frac{1}{n}\sum_{i=1}^n g(Z_i)
        } &\le 2\ERC_S(\G) + 3(b-a)\sqrt{
            \frac{\log 4/\delta}{2n}
        }
    \end{align*}

    \noindent With probability of at least $1-\delta$.
\end{proof*}


\subsection{Bounds for binary classification}
\textbf{Overview} : Given the following, 
\begin{itemize}
    \item $\X$ is a feature space and $\Y=\{-1, 1\}$ be a label space.
    \item $\Hf\subset\{-1, 1\}^{\X}$ : A set of binary classifiers.
    \item $\G=\bigCurl{ g_h : \X\times\Y \to \{0,1\} \Big| g_h(x, y) = \1{h(x)\ne y}, h\in\Hf }$ : Every function $g_h\in\G$ is the risk function of $h\in\Hf$.
    \item $S=\{(X_i, Y_i)\}_{i=1}^n\subset \X\times \Y$ is a sample dataset.
\end{itemize}

\begin{lemma}{$\ERC_S(\G) = \frac{1}{2}\ERC_S(\Hf)$}{rh_equals_half_rg}
    We have the following equality:
    \begin{align*}
        \ERC_S(\G) = \frac{1}{2}\ERC_S(\Hf)
    \end{align*}
\end{lemma}

\begin{proof*}[Lemma \ref{lem:rh_equals_half_rg}]
    From definition, we have:
    \begin{align*}
        \ERC_S(\G) 
            &= \E_\sigma\biggSquare{
                \sup_{h\in\Hf} \frac{1}{n} \sum_{i=1}^n \sigma_i \1{h(X_i)\ne Y_i} 
            } \\
            &= \E_\sigma\biggSquare{
                \sup_{h\in\Hf} \frac{1}{n} \sum_{i=1}^n \sigma_i \frac{1-Y_ih(X_i)}{2} 
            } \\
            &= \E_\sigma\biggSquare{
                \frac{1}{2n}\sum_{i=1}^n \sigma_i + \frac{1}{2}\sup_{h\in \Hf}\frac{1}{n} \sum_{i=1}^n \sigma_i(-Y_i)h(X_i)
            } \\ 
            &= \frac{1}{2n} \underbrace{\E_\sigma\biggSquare{
                \sum_{i=1}^n \sigma_i
            }}_{=0} + \frac{1}{2}\E_\sigma\biggSquare{
                \sup_{h\in \Hf}\frac{1}{n} \sum_{i=1}^n \sigma_i(-Y_i)h(X_i)
            } \\
            &= \frac{1}{2}\E_\sigma\biggSquare{
                \sup_{h\in \Hf}\frac{1}{n} \sum_{i=1}^n \sigma_i h(X_i)
            } \ \ \ (\sigma_i \text{ and } \sigma_i(-Y_i) \text{ has the same distribution}) \\
            &= \frac{1}{2}\ERC_S(\Hf)
    \end{align*}
\end{proof*}

\begin{corollary}{UDB for binary classification using Rademacher Complexity}{udb_bin_clf_rademacher}
    For all $\delta>0$, with probability of at least $1-\delta$, we have:
    \begin{align*}
        \sup_{h\in\Hf} \bigRound{
            R(h) - \widehat{R_n}(h)
        } &\le \RC_n(\Hf) + \sqrt{
            \frac{\log 1/\delta}{2n}
        } \\
        \sup_{h\in\Hf} \bigRound{
            R(h) - \widehat{R_n}(h)
        } &\le \ERC_S(\Hf) + 3\sqrt{
            \frac{\log 2/\delta}{2n}
        }
    \end{align*}

    \noindent We can also derive a two-sided UDB by replacing $\delta$ with $\delta/2$:
    \begin{align*}
        \sup_{h\in\Hf} \bigAbs{
            R(h) - \widehat{R_n}(h)
        } &\le \RC_n(\Hf) + \sqrt{
            \frac{\log 2/\delta}{2n}
        } \\
        \sup_{h\in\Hf} \bigAbs{
            R(h) - \widehat{R_n}(h)
        } &\le \ERC_S(\Hf) + 3\sqrt{
            \frac{\log 4/\delta}{2n}
        }
    \end{align*}
\end{corollary}

\begin{proof*}[Corollary \ref{coro:udb_bin_clf_rademacher}]
    Define the following function space:
    \begin{align*}
        \G = \bigCurl{
            g_h : \X \times \Y \to \{0,1\} \Big| g_h(x, y) = \1{h(x)\ne y}, h\in\Hf
        }
    \end{align*}

    \noindent By theorem \ref{thm:one_sided_rademacher_bound}, we have:
    \begin{align*}
        \sup_{g_h\in\G} \bigRound{\E[g_h] - \widehat{E_S}[g_h]}
            &= \sup_{h\in\Hf} \bigRound{\E[\1{h(X)\ne Y}] - \widehat{E_S}[\1{h(X)\ne Y}]} \\
            &= \sup_{h\in\Hf} \bigRound{R(h) - \widehat{R_n}(h)} \\
            &\le 2\RC_n(\G) + \sqrt{\frac{\log 1/\delta}{2n}} \ \ \ (\text{Theorem \ref{thm:one_sided_rademacher_bound}}) \\
            &= \RC_n(\Hf) + \sqrt{\frac{\log 1/\delta}{2n}} \ \ \ \biggRound{
                \text{Since } \ERC_S(\G) = \frac{1}{2}\ERC_S(\Hf)
            }
    \end{align*}

    \noindent Similarly, from theorem \ref{thm:one_sided_rademacher_bound}, we can also derive the following inequality:
    \begin{align*}
        \sup_{h\in\Hf} \bigRound{R(h) - \widehat{R_n}(h)} \le \ERC_S(\Hf) + 3\sqrt{\frac{\log 2/\delta}{2n}}
    \end{align*}

    \noindent By replacing $\delta$ with $\delta/2$, following the same arguments, we obtain the two-sided bounds.
\end{proof*}

\subsection{Proof of VC Inequality}
\label{sec:proof_of_vc_inequality}

In the following section, we will prove a slightly looser VC bound compared to theorem \ref{thm:vc_theorem_for_clf}. Then, we go ahead to improve the bound by improving the constant at the exponent at the expense of larger term in front of the exponential.

\begin{theorem}{One-sided VC Inequality}{one_sided_vc_inequality}
    For $0 < \delta < 1$, with probability of at least $1-\delta$, we have:
    \begin{align*}
        \sup_{h\in\Hf} \bigRound{
            R(h) - \widehat{R_n}(h)
        } \le \sqrt{
            \frac{8(\log S_\Hf(n) + \log(1/\delta))}{n}
        }
    \end{align*}

    \noindent Equivalently, for all $\epsilon>0$, we have:
    \begin{align*}
        P\biggRound{
            \sup_{h\in\Hf} \bigRound{
                R(h) - \widehat{R_n}(h)
            } \ge \epsilon
        } \le S_\Hf(n)e^{-n\epsilon^2/8}
    \end{align*}
\end{theorem}

\begin{proof*}[Theorem \ref{thm:one_sided_vc_inequality}]
    Let $\Hf\subset\{-1,1\}^\X$ and $S=(X_1, \dots, X_n)$ be a random sample. Denote the restriction of $\Hf$ to $S$ as:
    \begin{align*}
        \Hf_S = \bigCurl{
            (h(X_1), \dots, h(X_n)) : h \in \Hf
        }
    \end{align*}

    \noindent Hence, for any $u\in\Hf_S$, we have $\|u\|_2 = \sqrt{n}$. By Massart's lemma \ref{thm:massart_lemma}, we have:
    \begin{align*}
        \RC_n(\Hf) &= \E_S\E_\sigma\biggSquare{
            \sup_{u\in \Hf_S} \frac{1}{n}\sum_{i=1}^n \sigma_i u_i
        } \\
        &\le \E_S\biggSquare{
            \frac{\sup_{u\in\Hf} \|u\|_2 \sqrt{2\log |\Hf_S|}}{n}
        } \ \ \ (\text{Massart's lemma \ref{thm:massart_lemma}}) \\
        &=  \E_S\biggSquare{
            \frac{\sqrt{n}\sqrt{2\log |\Hf_S|}}{n}
        } =  \E_S\biggSquare{
            \sqrt{\frac{2\log |\Hf_S|}{n}}
        } \\
        &\le \sqrt{\frac{2\log \E[|\Hf_S|]}{n}} \ \ \ (\text{Jensen's inequality}) \\
        &\le \sqrt{\frac{2\log S_\Hf(n)}{n}} \ \ \ (|\Hf_S| \le S_\Hf(n))
    \end{align*}

    \noindent Combine the above inequality with corollary \ref{coro:udb_bin_clf_rademacher}, with probability of at least $1-\delta$, we have:
    \begin{align*}
        \sup_{h\in\Hf} \bigRound{
            R(h) - \widehat{R_n}(h)
        } &\le \RC_n(\Hf) + \sqrt{
            \frac{\log 1/\delta}{2n}
        } \\
        &\le \sqrt{\frac{2\log S_\Hf(n)}{n}} + \sqrt{\frac{\log 1/\delta}{2n}} \\
        &\le 2\sqrt{
            \frac{2\log S_\Hf(n)}{n} + \frac{\log 1/\delta}{2n}
        } \ \ \ (\sqrt{a} + \sqrt{b} \le 2\sqrt{a+b}) \\
        &= \sqrt{
            \frac{8(\log S_\Hf(n) + (\log 1/\delta)/4)}{n}
        }  \tag{*} \\
        &\le \sqrt{
            \frac{8(\log S_\Hf(n) + \log 1/\delta)}{n}
        }
    \end{align*}

    \noindent Now we set:
    \begin{align*}
        \epsilon = \sqrt{
            \frac{8(\log S_\Hf(n) + \log 1/\delta)}{n}
        } \implies \delta = S_\Hf(n)e^{-n\epsilon^2/8}
    \end{align*}

    \noindent Hence, for all $\epsilon > 0$, we have:
    \begin{align*}
        P\biggRound{
            \sup_{h\in\Hf} \bigRound{
                R(h) - \widehat{R_n}(h)
            } \ge \epsilon
        } \le S_\Hf(n)e^{-n\epsilon^2/8}
    \end{align*}
\end{proof*}

\begin{theorem}{Two-sided VC Inequality}{two_sided_vc_inequality}
    For $0<\delta<1$, with probability of at least $1-\delta$, we have:
    \begin{align*}
        \sup_{h\in\Hf} \bigAbs{
            R(h) - \widehat{R_n}(h)
        } \le \sqrt{
            \frac{8(\ln S_\Hf(n) + \ln(2/\delta))}{n}
        }
    \end{align*}

    \noindent Equivalently, for any $\epsilon>0$, 
    \begin{align*}
        P\biggRound{
            \sup_{h\in\Hf} \bigAbs{
                R(h) - \widehat{R_n}(h)
            }\ge \epsilon
        }\le 2S_\Hf(n)e^{-n\epsilon^2/8}
    \end{align*}
\end{theorem}

\begin{proof*}[Theorem \ref{thm:two_sided_vc_inequality}]
    Repeating the same arguments as theorem \ref{thm:one_sided_rademacher_bound}, we can also prove that:
    \begin{align*}
        P\biggRound{
            \inf_{h\in\Hf} \bigRound{
                R(h) - \widehat{R_n}(h)
            } \le -\epsilon 
        } \le S_\Hf(n)e^{-n\epsilon^2/8}
    \end{align*}

    \noindent Hence, combining the bounds, we have:
    \begin{align*}
        P\biggRound{
            \sup_{h\in\Hf} \bigAbs{
                R(h) - \widehat{R_n}(h)
            } \ge \epsilon 
        } 
        &\le P\biggRound{
            \inf_{h\in\Hf} \bigRound{
                R(h) - \widehat{R_n}(h)
            } \le -\epsilon 
        } + P\biggRound{
            \sup_{h\in\Hf} \bigRound{
                R(h) - \widehat{R_n}(h)
            } \ge \epsilon 
        } \\
        &\le 2S_\Hf(n)e^{-n\epsilon^2/8}
    \end{align*}
\end{proof*}

\noindent Now we proceed to prove theorem \ref{thm:vc_theorem_for_clf} by modifying the proof for the one-sided VC inequality (theorem \ref{thm:one_sided_vc_inequality}) starting from the inequality $(*)$. The idea is to improve the power of the exponent at the expense of the constant in front of the exponent.

\begin{proof*}[Theorem \ref{thm:vc_theorem_for_clf}]
    From theorem \ref{thm:one_sided_vc_inequality} proof, we have:
    \begin{align*}
        \sup_{h\in\Hf} \bigRound{ R(h) - \widehat{R_n}(h) }
        &\le \sqrt{
            \frac{8(\log S_\Hf(n) + (\log 1/\delta)/4)}{n}
        } \\
        &\le \sqrt{
            \frac{32(\log S_\Hf(n) + \log 1/\delta + \log 4)}{n}
        } \\
        &= \sqrt{
            \frac{32(\log4S_\Hf(n) + \log1/\delta)}{n}
        }
    \end{align*}

    \noindent Now, we set:
    \begin{align*}
        \epsilon = \sqrt{
            \frac{32(\log4S_\Hf(n) + \log1/\delta)}{n}
        } 
        &\implies \epsilon^2 = \frac{32(\log4S_\Hf(n) + \log1/\delta)}{n} \\
        &\implies \log\frac{4S_\Hf(n)}{\delta} = \frac{n\epsilon^2}{32} \\
        &\implies \log\frac{\delta}{4S_\Hf(n)} = -\frac{n\epsilon^2}{32} \\
        &\implies \frac{\delta}{4S_\Hf(n)} = e^{-n\epsilon^2/32} \\
        &\implies \delta = 4S_\Hf(n)e^{-n\epsilon^2/32}
    \end{align*}

    \noindent Therefore, for all $\epsilon > 0$, we have:
    \begin{align*}
        P\biggRound{
            \sup_{h\in\Hf} \bigRound{
                R(h) - \widehat{R_n}(h)
            } \ge \epsilon
        } \le 4S_\Hf(n)e^{-n\epsilon^2/32}
    \end{align*}

    \noindent Using the same arguments, we have:
    \begin{align*}
        P\biggRound{
            \inf_{h\in\Hf} \bigRound{
                R(h) - \widehat{R_n}(h)
            } \le -\epsilon
        } \le 4S_\Hf(n)e^{-n\epsilon^2/32}
    \end{align*}

    \noindent Hence, we obtain the following two-sided bound:
    \begin{align*}
        P\biggRound{
            \sup_{h\in\Hf}\bigAbs{
                R(h) - \widehat{R_n}(h)
            } \ge \epsilon
        } \le 8S_\Hf(n)e^{-n\epsilon^2/32}
    \end{align*}
\end{proof*}

\newpage
\subsection{End of chapter exercises}
\input{sections/main/exercises/chap6_exercises.tex}